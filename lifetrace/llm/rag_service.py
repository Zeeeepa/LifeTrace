import asyncio
from collections.abc import Generator
from datetime import datetime
from typing import Any

from lifetrace.llm.context_builder import ContextBuilder
from lifetrace.llm.llm_client import LLMClient
from lifetrace.llm.retrieval_service import RetrievalService
from lifetrace.storage import chat_mgr, project_mgr, task_mgr
from lifetrace.util.config import config
from lifetrace.util.logging_config import get_logger
from lifetrace.util.prompt_loader import get_prompt
from lifetrace.util.query_parser import QueryConditions, QueryParser

logger = get_logger()


class RAGService:
    """RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æœåŠ¡ï¼Œæ•´åˆæŸ¥è¯¢è§£æã€æ•°æ®æ£€ç´¢ã€ä¸Šä¸‹æ–‡æ„å»ºå’ŒLLMç”Ÿæˆ"""

    def __init__(self):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        """
        self.llm_client = LLMClient()
        self.retrieval_service = RetrievalService()
        self.context_builder = ContextBuilder()
        self.query_parser = QueryParser(self.llm_client)

        logger.info("RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def process_query(self, user_query: str, max_results: int = 50) -> dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´RAGæµæ°´çº¿

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            max_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡

        Returns:
            åŒ…å«ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        start_time = datetime.now()

        try:
            # 1. æ„å›¾è¯†åˆ«
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            intent_result = self.llm_client.classify_intent(user_query)

            # å¦‚æœä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨LLMç”Ÿæˆå›å¤
            if not intent_result.get("needs_database", True):
                logger.info(f"ç”¨æˆ·æ„å›¾ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢: {intent_result['intent_type']}")
                if self.llm_client.is_available():
                    response_text = self._generate_direct_response(user_query, intent_result)
                else:
                    response_text = self._fallback_direct_response(user_query, intent_result)

                processing_time = (datetime.now() - start_time).total_seconds()
                return {
                    "success": True,
                    "response": response_text,
                    "query_info": {
                        "original_query": user_query,
                        "intent_classification": intent_result,
                        "requires_database": False,
                    },
                    "performance": {
                        "processing_time_seconds": processing_time,
                        "timestamp": start_time.isoformat(),
                    },
                }

            # 2. æŸ¥è¯¢è§£æï¼ˆä»…å½“éœ€è¦æ•°æ®åº“æŸ¥è¯¢æ—¶ï¼‰
            logger.info("éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼Œå¼€å§‹æŸ¥è¯¢è§£æ")
            parsed_query = self.query_parser.parse_query(user_query)
            # ç¡®å®šæŸ¥è¯¢ç±»å‹
            query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"

            # 3. æ•°æ®æ£€ç´¢ - ä½¿ç”¨å·²è§£æçš„æŸ¥è¯¢æ¡ä»¶ï¼Œé¿å…é‡å¤è§£æ
            logger.info("å¼€å§‹æ•°æ®æ£€ç´¢")
            logger.info(f"è§£æåçš„æŸ¥è¯¢æ¡ä»¶: {parsed_query}")

            retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, max_results)

            # 4. è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            stats = None
            if query_type == "statistics" or "ç»Ÿè®¡" in user_query:
                # å®‰å…¨åœ°è®¿é—®parsed_queryçš„å±æ€§
                if isinstance(parsed_query, QueryConditions):
                    start_date = parsed_query.start_date
                    end_date = parsed_query.end_date
                    app_names = parsed_query.app_names
                    keywords = parsed_query.keywords or []
                else:
                    # å¦‚æœparsed_queryæ˜¯å­—å…¸ï¼Œä»å­—å…¸ä¸­è·å–å€¼
                    start_date = parsed_query.get("start_date")
                    end_date = parsed_query.get("end_date")
                    app_names = parsed_query.get("app_names", [])
                    keywords = parsed_query.get("keywords", [])

                conditions = QueryConditions(
                    start_date=start_date,
                    end_date=end_date,
                    app_names=app_names,
                    keywords=keywords,
                )
                stats = self.retrieval_service.get_statistics(conditions)

            # 5. ä¸Šä¸‹æ–‡æ„å»º
            logger.info("å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡")
            if query_type == "statistics":
                context_text = self.context_builder.build_statistics_context(
                    user_query, retrieved_data, stats
                )
            elif query_type == "search":
                context_text = self.context_builder.build_search_context(user_query, retrieved_data)
            else:
                context_text = self.context_builder.build_summary_context(
                    user_query, retrieved_data
                )

            # 6. LLMç”Ÿæˆ
            logger.info("å¼€å§‹LLMç”Ÿæˆ")
            if self.llm_client.is_available():
                response_text = self.llm_client.generate_summary(user_query, retrieved_data)
            else:
                response_text = self._fallback_response(user_query, retrieved_data, stats)

            # 7. æ„å»ºå“åº”
            processing_time = (datetime.now() - start_time).total_seconds()

            result = {
                "success": True,
                "response": response_text,
                "query_info": {
                    "original_query": user_query,
                    "intent_classification": intent_result,
                    "parsed_query": parsed_query,
                    "query_type": query_type,
                    "requires_database": True,
                },
                "retrieval_info": {
                    "total_found": len(retrieved_data),
                    "data_summary": self._summarize_retrieved_data(retrieved_data),
                },
                "context_info": {
                    "context_length": len(context_text),
                    "llm_available": self.llm_client.is_available(),
                },
                "performance": {
                    "processing_time_seconds": processing_time,
                    "timestamp": start_time.isoformat(),
                },
                "statistics": stats,
            }

            logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            return result

        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            # å®‰å…¨åœ°æ„å»ºé”™è¯¯ä¿¡æ¯
            error_query_info = {"original_query": user_query}
            try:
                if "parsed_query" in locals():
                    error_query_info["error"] = str(e)
            except:  # noqa: E722
                pass

            return {
                "success": False,
                "error": str(e),
                "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°äº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚",
                "query_info": error_query_info,
                "performance": {
                    "processing_time_seconds": (datetime.now() - start_time).total_seconds(),
                    "timestamp": start_time.isoformat(),
                },
            }

    def process_query_sync(self, user_query: str, max_results: int = 50) -> dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æŸ¥è¯¢å¤„ç†

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            max_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡

        Returns:
            åŒ…å«ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        return asyncio.run(self.process_query(user_query, max_results))

    def post_stream_decision(self, user_query: str, output_text: str) -> None:
        """
        æµå¼è¾“å‡ºå®Œæˆåçš„åˆ¤å®š/è®°å½•é’©å­ï¼š
        - ç”¨äºæ‰§è¡Œé‚£äº›â€œå¿…é¡»æ‹¿åˆ°å®Œæ•´è¾“å‡ºæ‰èƒ½åˆ¤æ–­â€çš„é€»è¾‘ï¼ˆä¾‹å¦‚ï¼šæ˜¯å¦éœ€è¦è¿½åŠ å…è´£å£°æ˜ã€æ˜¯å¦è§¦å‘æŸäº›åç»­åŠ¨ä½œç­‰ï¼‰
        - é»˜è®¤å®ç°ä»…åšæ—¥å¿—è®°å½•ï¼Œåç»­å¯æŒ‰éœ€æ‰©å±•
        """
        try:
            if not output_text:
                return
            # ç¤ºä¾‹ï¼šå¦‚æœè¾“å‡ºåŒ…å«ç‰¹å®šæç¤ºè¯ï¼Œåˆ™è®°å½•åˆ°æ—¥å¿—æˆ–è§¦å‘åç»­å¤„ç†
            keywords = ["å…è´£å£°æ˜", "æ•æ„Ÿå†…å®¹", "æ³¨æ„", "æ€»ç»“"]
            if any(kw in output_text for kw in keywords):
                logger.info(
                    f"[post_stream] è¾“å‡ºåŒ…å«å…³é”®æç¤ºï¼Œquery='{user_query[:50]}...' è§¦å‘æ ‡è®°"
                )
            else:
                logger.debug("[post_stream] æ— ç‰¹æ®Šæ ‡è®°")
        except Exception as e:
            logger.debug(f"[post_stream] å¤„ç†å¼‚å¸¸å·²å¿½ç•¥: {e}")

    def stream_query(
        self,
        user_query: str,
        max_results: int = 50,
        temperature_direct: float = 0.7,
        temperature_rag: float = 0.3,
    ) -> Generator[str]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼šæ‰§è¡Œå®Œæ•´çš„RAGæµç¨‹ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µé€tokenï¼ˆæˆ–é€chunkï¼‰yield æ–‡æœ¬ã€‚
        å½“åº•å±‚LLMä¸æ”¯æŒçœŸæµå¼æ—¶ï¼Œå°†æŒ‰æ®µè¿”å›ï¼›å½“ä¸å¯ç”¨æ—¶ï¼Œè¿”å›å¤‡ç”¨æ–‡æœ¬ã€‚
        åœ¨æµå¼è¾“å‡ºå®Œæˆåï¼Œè°ƒç”¨ post_stream_decision è¿›è¡Œåç»­åˆ¤å®š/è®°å½•ã€‚
        """
        try:
            # 1) æ„å›¾è¯†åˆ«
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            # 2) ä¸éœ€è¦æ•°æ®åº“ï¼šç›´æ¥å¯¹è¯
            if not needs_db:
                if not self.llm_client.is_available():
                    # LLMä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›å¤‡ç”¨æ–‡æœ¬
                    fallback_text = self._fallback_direct_response(user_query, intent_result)
                    yield fallback_text
                    # å®Œæ•´è¾“å‡ºåå¤„ç†
                    self.post_stream_decision(user_query, fallback_text)
                    return
                # ç³»ç»Ÿæç¤ºä¸ _generate_direct_response ä¿æŒä¸€è‡´
                intent_type = intent_result.get("intent_type", "general_chat")
                if intent_type == "system_help":
                    system_prompt = get_prompt("rag", "system_help")
                else:
                    system_prompt = get_prompt("rag", "general_chat")
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ]
                output_chunks: list[str] = []
                for text in self.llm_client.stream_chat(
                    messages=messages, temperature=temperature_direct
                ):
                    if text:
                        output_chunks.append(text)
                        yield text
                # å®Œæ•´è¾“å‡ºåå¤„ç†
                self.post_stream_decision(user_query, "".join(output_chunks))
                return

            # 3) éœ€è¦æ•°æ®åº“ï¼šè§£æ + æ£€ç´¢ + æ„å»ºä¸Šä¸‹æ–‡
            parsed_query = self.query_parser.parse_query(user_query)
            query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
            retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, max_results)

            stats = None
            if query_type == "statistics" or "ç»Ÿè®¡" in user_query:
                # å…¼å®¹ QueryConditions æˆ– dict
                if isinstance(parsed_query, QueryConditions):
                    conditions = parsed_query
                else:
                    conditions = QueryConditions(
                        start_date=parsed_query.get("start_date"),
                        end_date=parsed_query.get("end_date"),
                        app_names=parsed_query.get("app_names"),
                        keywords=parsed_query.get("keywords", []),
                    )
                try:
                    stats = self.retrieval_service.get_statistics(conditions)
                except Exception:
                    stats = None

            # ä¸Šä¸‹æ–‡æ„å»º
            if query_type == "statistics":
                context_text = self.context_builder.build_statistics_context(
                    user_query, retrieved_data, stats
                )
            elif query_type == "search":
                context_text = self.context_builder.build_search_context(user_query, retrieved_data)
            else:
                context_text = self.context_builder.build_summary_context(
                    user_query, retrieved_data
                )

            # LLM ä¸å¯ç”¨æ—¶ï¼Œè¿”å›è§„åˆ™å¤‡é€‰
            if not self.llm_client.is_available():
                fallback_text = self._fallback_response(user_query, retrieved_data, stats)
                yield fallback_text
                # å®Œæ•´è¾“å‡ºåå¤„ç†
                self.post_stream_decision(user_query, fallback_text)
                return

            # 4) ç”Ÿæˆé˜¶æ®µï¼šæµå¼è¾“å‡º
            system_prompt = get_prompt("rag", "history_analysis")
            user_prompt = get_prompt(
                "rag", "user_query_template", query=user_query, context=context_text
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            output_chunks: list[str] = []
            for text in self.llm_client.stream_chat(messages=messages, temperature=temperature_rag):
                if text:
                    output_chunks.append(text)
                    yield text
            # å®Œæ•´è¾“å‡ºåå¤„ç†
            self.post_stream_decision(user_query, "".join(output_chunks))
        except Exception as e:
            logger.error(f"RAG æµå¼å¤„ç†å¤±è´¥: {e}")
            error_text = "\n[æç¤º] æµå¼å¤„ç†å‡ºç°å¼‚å¸¸ï¼Œå·²ç»“æŸã€‚"
            yield error_text
            # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿåšä¸€æ¬¡åå¤„ç†
            try:
                self.post_stream_decision(user_query, error_text)
            except Exception:
                pass

    def get_query_suggestions(self, partial_query: str = "") -> list[str]:
        """
        è·å–æŸ¥è¯¢å»ºè®®

        Args:
            partial_query: éƒ¨åˆ†æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æŸ¥è¯¢å»ºè®®åˆ—è¡¨
        """
        suggestions = [
            "æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©è®°å½•",
            "æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„æ‰€æœ‰è®°å½•",
            "ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨å„åº”ç”¨çš„ä½¿ç”¨æƒ…å†µ",
            "æœç´¢æ˜¨å¤©æµè§ˆå™¨ä¸­çš„å†…å®¹",
            "æ€»ç»“æœ€è¿‘çš„å·¥ä½œç›¸å…³æˆªå›¾",
            "æŸ¥æ‰¾åŒ…å«'é¡¹ç›®'å…³é”®è¯çš„è®°å½•",
            "ç»Ÿè®¡æœ¬æœˆQQèŠå¤©è®°å½•æ•°é‡",
            "æœç´¢æœ€è¿‘3å¤©çš„å­¦ä¹ èµ„æ–™",
            "æ€»ç»“ä¸Šå‘¨çš„ç½‘é¡µæµè§ˆè®°å½•",
            "æŸ¥æ‰¾åŒ…å«'æ–‡æ¡£'çš„æ‰€æœ‰åº”ç”¨è®°å½•",
        ]

        if partial_query:
            # ç®€å•çš„æ¨¡ç³ŠåŒ¹é…
            filtered_suggestions = [
                s for s in suggestions if any(word in s for word in partial_query.split())
            ]
            return filtered_suggestions[:5]

        return suggestions[:5]

    def get_supported_query_types(self) -> dict[str, Any]:
        """
        è·å–æ”¯æŒçš„æŸ¥è¯¢ç±»å‹ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç±»å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "query_types": {
                "summary": {
                    "name": "æ€»ç»“",
                    "description": "å¯¹å†å²è®°å½•è¿›è¡Œæ€»ç»“å’Œæ¦‚æ‹¬",
                    "examples": ["æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©", "æ¦‚æ‹¬æœ€è¿‘çš„å·¥ä½œè®°å½•"],
                },
                "search": {
                    "name": "æœç´¢",
                    "description": "æœç´¢åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®°å½•",
                    "examples": ["æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„è®°å½•", "æœç´¢é¡¹ç›®ç›¸å…³å†…å®¹"],
                },
                "statistics": {
                    "name": "ç»Ÿè®¡",
                    "description": "ç»Ÿè®¡å’Œåˆ†æå†å²è®°å½•æ•°æ®",
                    "examples": ["ç»Ÿè®¡å„åº”ç”¨ä½¿ç”¨æƒ…å†µ", "åˆ†ææœ€è¿‘ä¸€å‘¨çš„æ´»åŠ¨"],
                },
            },
            "supported_apps": [
                "WeChat",
                "QQ",
                "Browser",
                "Chrome",
                "Firefox",
                "Edge",
                "Word",
                "Excel",
                "PowerPoint",
                "Notepad",
                "VSCode",
            ],
            "time_expressions": [
                "ä»Šå¤©",
                "æ˜¨å¤©",
                "æœ€è¿‘3å¤©",
                "æœ¬å‘¨",
                "ä¸Šå‘¨",
                "æœ¬æœˆ",
                "ä¸Šæœˆ",
            ],
        }

    def _summarize_retrieved_data(self, retrieved_data: list[dict[str, Any]]) -> dict[str, Any]:
        """æ€»ç»“æ£€ç´¢åˆ°çš„æ•°æ®"""
        if not retrieved_data:
            return {"apps": {}, "time_range": None, "total": 0}

        app_counts = {}
        timestamps = []

        for record in retrieved_data:
            app_name = record.get("app_name", "æœªçŸ¥åº”ç”¨")
            app_counts[app_name] = app_counts.get(app_name, 0) + 1

            timestamp = record.get("timestamp")
            if timestamp:
                timestamps.append(timestamp)

        time_range = None
        if timestamps:
            timestamps.sort()
            time_range = {"earliest": timestamps[0], "latest": timestamps[-1]}

        return {
            "apps": app_counts,
            "time_range": time_range,
            "total": len(retrieved_data),
        }

    def _fallback_response(
        self,
        user_query: str,
        retrieved_data: list[dict[str, Any]],
        stats: dict[str, Any] = None,
    ) -> str:
        """
        å¤‡ç”¨å“åº”ç”Ÿæˆï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_data: æ£€ç´¢åˆ°çš„æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¤‡ç”¨å“åº”æ–‡æœ¬
        """
        if not retrieved_data:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æŸ¥è¯¢ '{user_query}' ç›¸å…³çš„å†å²è®°å½•ã€‚"

        response_parts = [f"æ ¹æ®æ‚¨çš„æŸ¥è¯¢ '{user_query}'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š", ""]

        # åŸºç¡€ç»Ÿè®¡
        response_parts.append(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(retrieved_data)} æ¡ç›¸å…³è®°å½•")

        # åº”ç”¨åˆ†å¸ƒ
        app_summary = self._summarize_retrieved_data(retrieved_data)
        if app_summary["apps"]:
            response_parts.append("\nğŸ“± åº”ç”¨åˆ†å¸ƒï¼š")
            for app, count in sorted(app_summary["apps"].items(), key=lambda x: x[1], reverse=True):
                response_parts.append(f"  â€¢ {app}: {count} æ¡è®°å½•")

        # æ—¶é—´èŒƒå›´
        if app_summary["time_range"]:
            try:
                earliest = datetime.fromisoformat(
                    app_summary["time_range"]["earliest"].replace("Z", "+00:00")
                )
                latest = datetime.fromisoformat(
                    app_summary["time_range"]["latest"].replace("Z", "+00:00")
                )
                response_parts.append(
                    f"\nâ° æ—¶é—´èŒƒå›´: {earliest.strftime('%Y-%m-%d %H:%M')} è‡³ {latest.strftime('%Y-%m-%d %H:%M')}"
                )
            except:  # noqa: E722
                pass

        # æœ€æ–°è®°å½•ç¤ºä¾‹
        if retrieved_data:
            response_parts.append("\nğŸ“ æœ€æ–°è®°å½•ç¤ºä¾‹ï¼š")
            latest_record = retrieved_data[0]
            timestamp = latest_record.get("timestamp", "æœªçŸ¥æ—¶é—´")
            app_name = latest_record.get("app_name", "æœªçŸ¥åº”ç”¨")
            ocr_text = latest_record.get("ocr_text", "æ— å†…å®¹")[:100]

            response_parts.append(f"  æ—¶é—´: {timestamp}")
            response_parts.append(f"  åº”ç”¨: {app_name}")
            response_parts.append(f"  å†…å®¹: {ocr_text}...")

        response_parts.append("\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯æ¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœã€‚")

        return "\n".join(response_parts)

    def health_check(self) -> dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥

        Returns:
            æœåŠ¡çŠ¶æ€ä¿¡æ¯
        """
        return {
            "rag_service": "healthy",
            "llm_client": ("available" if self.llm_client.is_available() else "unavailable"),
            "database": "connected",
            "components": {
                "retrieval_service": "ready",
                "context_builder": "ready",
                "query_parser": "ready",
            },
            "timestamp": datetime.now().isoformat(),
        }

    def _generate_direct_response(self, user_query: str, intent_result: dict[str, Any]) -> str:
        """
        ä¸ºä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„ç”¨æˆ·è¾“å…¥ç”Ÿæˆç›´æ¥å›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        try:
            intent_type = intent_result.get("intent_type", "general_chat")

            if intent_type == "system_help":
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ã€‚LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æˆªå›¾è®°å½•ç”¨æˆ·çš„å±å¹•æ´»åŠ¨
2. OCRæ–‡å­—è¯†åˆ«å’Œå†…å®¹åˆ†æ
3. åº”ç”¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡
4. æ™ºèƒ½æœç´¢å’ŒæŸ¥è¯¢åŠŸèƒ½

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰ç”¨çš„å¸®åŠ©ä¿¡æ¯ã€‚
"""
            else:
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ä»¥å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
å¦‚æœç”¨æˆ·éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å¼•å¯¼ä»–ä»¬ä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢è¯­å¥ã€‚
"""

            response = self.llm_client.client.chat.completions.create(
                model=self.llm_client.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ],
                temperature=0.7,
                max_tokens=500,
            )

            # è®°å½•LLMå“åº”åˆ°æ—¥å¿—
            llm_response = response.choices[0].message.content.strip()
            logger.info(f"[LLM Direct Response] {llm_response}")
            logger.info(f"LLMç›´æ¥å“åº”: {llm_response}")

            return llm_response

        except Exception as e:
            logger.error(f"ç›´æ¥å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_direct_response(user_query, intent_result)

    def _fallback_direct_response(self, user_query: str, intent_result: dict[str, Any]) -> str:
        """
        å½“LLMä¸å¯ç”¨æ—¶çš„ç›´æ¥å›å¤å¤‡ç”¨æ–¹æ¡ˆ

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            å¤‡ç”¨å›å¤æ–‡æœ¬
        """
        intent_type = intent_result.get("intent_type", "general_chat")

        if intent_type == "system_help":
            return """
LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

ğŸ“¸ **è‡ªåŠ¨æˆªå›¾è®°å½•**
- å®šæœŸæ•è·å±å¹•å†…å®¹
- è®°å½•åº”ç”¨ä½¿ç”¨æƒ…å†µ

ğŸ” **æ™ºèƒ½æœç´¢**
- æœç´¢å†å²æˆªå›¾
- åŸºäºOCRæ–‡å­—å†…å®¹æŸ¥æ‰¾

ğŸ“Š **ä½¿ç”¨ç»Ÿè®¡**
- åº”ç”¨ä½¿ç”¨æ—¶é•¿ç»Ÿè®¡
- æ´»åŠ¨æ¨¡å¼åˆ†æ

ğŸ’¬ **æ™ºèƒ½é—®ç­”**
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- ä¸ªæ€§åŒ–æ•°æ®åˆ†æ

å¦‚éœ€æŸ¥è¯¢å…·ä½“æ•°æ®ï¼Œè¯·ä½¿ç”¨å¦‚"æœç´¢åŒ…å«ç¼–ç¨‹çš„æˆªå›¾"æˆ–"ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨çš„åº”ç”¨ä½¿ç”¨æƒ…å†µ"ç­‰è¯­å¥ã€‚
"""
        elif intent_type == "general_chat":
            greetings = [
                "ä½ å¥½ï¼æˆ‘æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼",
                "æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "æ¬¢è¿ä½¿ç”¨LifeTraceï¼æˆ‘å¯ä»¥å¸®æ‚¨æŸ¥è¯¢å’Œåˆ†ææ‚¨çš„ç”Ÿæ´»è½¨è¿¹æ•°æ®ã€‚",
            ]

            if any(word in user_query.lower() for word in ["ä½ å¥½", "hello", "hi"]):
                return greetings[0] + "\n\næ‚¨å¯ä»¥è¯¢é—®æˆ‘å…³äºLifeTraceçš„åŠŸèƒ½ï¼Œæˆ–è€…ç›´æ¥æŸ¥è¯¢æ‚¨çš„æ•°æ®ã€‚"
            elif any(word in user_query.lower() for word in ["è°¢è°¢", "thanks"]):
                return "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ã€‚"
            else:
                return greetings[1] + "\n\næ‚¨å¯ä»¥å°è¯•æœç´¢æˆªå›¾ã€æŸ¥è¯¢åº”ç”¨ä½¿ç”¨æƒ…å†µï¼Œæˆ–è€…è¯¢é—®ç³»ç»ŸåŠŸèƒ½ã€‚"
        else:
            return "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½æä¾›å‡†ç¡®çš„å›ç­”ã€‚æ‚¨å¯ä»¥å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢ï¼Œæ¯”å¦‚æœç´¢ç‰¹å®šå†…å®¹æˆ–ç»Ÿè®¡ä½¿ç”¨æƒ…å†µã€‚"

    async def process_query_stream(
        self,
        user_query: str,
        project_id: int | None = None,
        task_ids: list[int] | None = None,
        session_id: str | None = None,
    ) -> dict[str, Any]:
        """
        ä¸ºæµå¼æ¥å£å¤„ç†æŸ¥è¯¢ï¼Œè¿”å›æ„å»ºå¥½çš„messageså’Œtemperature
        é¿å…é‡å¤çš„æ„å›¾è¯†åˆ«è°ƒç”¨

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            project_id: å¯é€‰çš„é¡¹ç›®IDï¼Œç”¨äºè¿‡æ»¤ä¸Šä¸‹æ–‡
            task_ids: å¯é€‰çš„ä»»åŠ¡IDåˆ—è¡¨ï¼Œè¡¨ç¤ºé€‰ä¸­çš„ä»»åŠ¡
            session_id: å¯é€‰çš„ä¼šè¯IDï¼Œç”¨äºè·å–å†å²å¯¹è¯
        """
        try:
            # 1. æ„å›¾è¯†åˆ«
            logger.info(
                f"[stream] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}, project_id: {project_id}, task_ids: {task_ids}, session_id: {session_id}"
            )
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            messages = []
            temperature = 0.7

            # è·å–å†å²å¯¹è¯é…ç½®
            chat_config = config.get("chat", {})
            enable_history = chat_config.get("enable_history", True)
            history_limit = chat_config.get("history_limit", 10)
            logger.info(
                f"[stream] å†å²å¯¹è¯é…ç½®: enable_history={enable_history}, history_limit={history_limit}"
            )

            # è·å–é¡¹ç›®ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº† project_idï¼‰
            project_info = None
            tasks_info_str = "æš‚æ— ä»»åŠ¡"
            selected_tasks_info_str = None

            if project_id:
                project_info = project_mgr.get_project(project_id)
                logger.info(f"[stream] è·å–åˆ°é¡¹ç›®ä¿¡æ¯: {project_info}")

                # è·å–é¡¹ç›®çš„ä»»åŠ¡åˆ—è¡¨
                tasks = task_mgr.list_tasks(project_id, limit=100)
                if tasks:
                    # æ ¼å¼åŒ–ä»»åŠ¡ä¿¡æ¯
                    tasks_list = []
                    for task in tasks:
                        status_emoji = {
                            "pending": "â³",
                            "in_progress": "ğŸ”„",
                            "completed": "âœ…",
                            "cancelled": "âŒ",
                        }.get(task.get("status", "pending"), "ğŸ“")

                        task_line = f"{status_emoji} [{task.get('status', 'pending')}] {task.get('name', 'æœªå‘½åä»»åŠ¡')}"
                        if task.get("description"):
                            # é™åˆ¶æè¿°ä¸ºå‰50ä¸ªå­—ç¬¦
                            description = task.get("description")
                            if len(description) > 50:
                                description = description[:50] + "..."
                            task_line += f"\n   æè¿°: {description}"
                        tasks_list.append(task_line)

                    tasks_info_str = "\n".join(tasks_list)
                    logger.info(f"[stream] è·å–åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
                else:
                    logger.info(f"[stream] é¡¹ç›® {project_id} æš‚æ— ä»»åŠ¡")

                # å¦‚æœæä¾›äº†é€‰ä¸­çš„ä»»åŠ¡IDï¼Œè·å–è¿™äº›ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
                if task_ids and len(task_ids) > 0:
                    selected_tasks_list = []
                    for task_id in task_ids:
                        task = task_mgr.get_task(task_id)
                        if task:
                            status_emoji = {
                                "pending": "â³",
                                "in_progress": "ğŸ”„",
                                "completed": "âœ…",
                                "cancelled": "âŒ",
                            }.get(task.get("status", "pending"), "ğŸ“")

                            # é€‰ä¸­çš„ä»»åŠ¡æ˜¾ç¤ºå®Œæ•´æè¿°ï¼ˆä¸é™åˆ¶å­—ç¬¦ï¼‰
                            task_line = f"{status_emoji} [{task.get('status', 'pending')}] {task.get('name', 'æœªå‘½åä»»åŠ¡')}"
                            if task.get("description"):
                                task_line += f"\n   æè¿°: {task.get('description')}"
                            selected_tasks_list.append(task_line)

                    if selected_tasks_list:
                        selected_tasks_info_str = "\n\n".join(selected_tasks_list)
                        logger.info(f"[stream] è·å–åˆ° {len(selected_tasks_list)} ä¸ªé€‰ä¸­çš„ä»»åŠ¡")

            if not needs_db:
                # ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æƒ…å†µï¼ˆä¸ä¼šæ£€ç´¢å†å²æ•°æ®ï¼‰
                intent_type = intent_result.get("intent_type", "general_chat")

                # å¦‚æœæ˜¯é¡¹ç›®å¯¹è¯ï¼Œä½¿ç”¨é¡¹ç›®åŠ©æ‰‹æç¤ºè¯ï¼ˆæ— å†å²æ•°æ®ç‰ˆæœ¬ï¼‰
                if project_info:
                    # å¦‚æœæœ‰é€‰ä¸­çš„ä»»åŠ¡ï¼Œä½¿ç”¨å¸¦é€‰ä¸­ä»»åŠ¡çš„æç¤ºè¯
                    if selected_tasks_info_str:
                        system_prompt = get_prompt(
                            "project_assistant",
                            "system_prompt_with_selected_tasks",
                            project_name=project_info.get("name", "æœªå‘½åé¡¹ç›®"),
                            project_goal=project_info.get("goal", "æš‚æ— ç›®æ ‡æè¿°"),
                            selected_tasks_info=selected_tasks_info_str,
                            tasks_info=tasks_info_str,
                        )
                    else:
                        system_prompt = get_prompt(
                            "project_assistant",
                            "system_prompt",
                            project_name=project_info.get("name", "æœªå‘½åé¡¹ç›®"),
                            project_goal=project_info.get("goal", "æš‚æ— ç›®æ ‡æè¿°"),
                            tasks_info=tasks_info_str,
                        )
                elif intent_type == "system_help":
                    system_prompt = get_prompt("rag", "system_help")
                else:
                    system_prompt = get_prompt("rag", "general_chat")

                messages = [{"role": "system", "content": system_prompt}]

                # æ·»åŠ å†å²å¯¹è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if enable_history and session_id and history_limit > 0:
                    try:
                        # è·å–å†å²æ¶ˆæ¯ï¼Œé™åˆ¶æ•°é‡ä¸º history_limit * 2ï¼ˆå› ä¸º1è½®=ç”¨æˆ·+åŠ©æ‰‹ï¼‰
                        history_messages = chat_mgr.get_messages(
                            session_id, limit=history_limit * 2
                        )
                        # æŒ‰æ—¶é—´é¡ºåºæ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæ’é™¤systemæ¶ˆæ¯ï¼‰
                        for msg in history_messages:
                            if msg["role"] in ["user", "assistant"]:
                                messages.append({"role": msg["role"], "content": msg["content"]})
                        if history_messages:
                            logger.info(f"[stream] æ·»åŠ äº† {len(history_messages)} æ¡å†å²æ¶ˆæ¯")
                    except Exception as e:
                        logger.warning(f"[stream] è·å–å†å²æ¶ˆæ¯å¤±è´¥: {e}")

                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": user_query})
            else:
                # éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æƒ…å†µï¼ˆä¼šæ£€ç´¢å†å²æ•°æ®ï¼‰
                parsed_query = self.query_parser.parse_query(user_query)
                # å¦‚æœæä¾›äº† project_idï¼Œæ·»åŠ åˆ°æŸ¥è¯¢æ¡ä»¶ä¸­
                if project_id:
                    parsed_query.project_id = project_id
                query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
                retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, 500)

                # æ„å»ºä¸Šä¸‹æ–‡
                if query_type == "statistics":
                    stats = None
                    if isinstance(parsed_query, QueryConditions):
                        stats = self.retrieval_service.get_statistics(parsed_query)
                    context_text = self.context_builder.build_statistics_context(
                        user_query, retrieved_data, stats
                    )
                else:
                    context_text = self.context_builder.build_search_context(
                        user_query, retrieved_data
                    )
                logger.debug(f"æ„å»ºçš„ä¸Šä¸‹æ–‡å†…å®¹: {context_text}")

                # å¦‚æœæ˜¯é¡¹ç›®å¯¹è¯ï¼Œä½¿ç”¨å¸¦å†å²æ•°æ®çš„é¡¹ç›®åŠ©æ‰‹æç¤ºè¯
                if project_info:
                    # å¦‚æœæœ‰é€‰ä¸­çš„ä»»åŠ¡ï¼Œä½¿ç”¨å¸¦å†å²æ•°æ®å’Œé€‰ä¸­ä»»åŠ¡çš„æç¤ºè¯
                    if selected_tasks_info_str:
                        project_context = get_prompt(
                            "project_assistant",
                            "system_prompt_with_data_and_selected_tasks",
                            project_name=project_info.get("name", "æœªå‘½åé¡¹ç›®"),
                            project_goal=project_info.get("goal", "æš‚æ— ç›®æ ‡æè¿°"),
                            selected_tasks_info=selected_tasks_info_str,
                            tasks_info=tasks_info_str,
                        )
                    else:
                        project_context = get_prompt(
                            "project_assistant",
                            "system_prompt_with_data",
                            project_name=project_info.get("name", "æœªå‘½åé¡¹ç›®"),
                            project_goal=project_info.get("goal", "æš‚æ— ç›®æ ‡æè¿°"),
                            tasks_info=tasks_info_str,
                        )
                    # å°†é¡¹ç›®ä¸Šä¸‹æ–‡å’Œæ•°æ®ä¸Šä¸‹æ–‡ç»“åˆ
                    system_content = f"{project_context}\n\n{context_text}"
                else:
                    # éé¡¹ç›®å¯¹è¯ï¼Œä½¿ç”¨äº‹ä»¶åŠ©æ‰‹çš„æç¤ºè¯
                    system_content = context_text

                messages = [{"role": "system", "content": system_content}]

                # æ·»åŠ å†å²å¯¹è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if enable_history and session_id and history_limit > 0:
                    try:
                        # è·å–å†å²æ¶ˆæ¯ï¼Œé™åˆ¶æ•°é‡ä¸º history_limit * 2ï¼ˆå› ä¸º1è½®=ç”¨æˆ·+åŠ©æ‰‹ï¼‰
                        history_messages = chat_mgr.get_messages(
                            session_id, limit=history_limit * 2
                        )
                        # æŒ‰æ—¶é—´é¡ºåºæ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæ’é™¤systemæ¶ˆæ¯ï¼‰
                        for msg in history_messages:
                            if msg["role"] in ["user", "assistant"]:
                                messages.append({"role": msg["role"], "content": msg["content"]})
                        if history_messages:
                            logger.info(f"[stream] æ·»åŠ äº† {len(history_messages)} æ¡å†å²æ¶ˆæ¯")
                    except Exception as e:
                        logger.warning(f"[stream] è·å–å†å²æ¶ˆæ¯å¤±è´¥: {e}")

                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": user_query})
                temperature = 0.3

            return {
                "success": True,
                "messages": messages,
                "temperature": temperature,
                "intent_result": intent_result,
            }

        except Exception as e:
            logger.error(f"[stream] å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "response": f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "messages": [],
                "temperature": 0.7,
            }
