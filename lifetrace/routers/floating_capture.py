"""æ‚¬æµ®çª—æˆªå›¾å¾…åŠæå–è·¯ç”±"""

import json
import re
import time
from functools import lru_cache
from typing import TYPE_CHECKING, Any, cast

from fastapi import APIRouter, HTTPException

if TYPE_CHECKING:
    from openai.types.chat import ChatCompletionMessageParam
else:
    ChatCompletionMessageParam = Any

from lifetrace.llm.llm_client import LLMClient
from lifetrace.schemas.floating_capture import (
    CreatedTodo,
    ExtractedTodo,
    FloatingCaptureRequest,
    FloatingCaptureResponse,
)
from lifetrace.storage import todo_mgr
from lifetrace.util.logging_config import get_logger
from lifetrace.util.prompt_loader import get_prompt
from lifetrace.util.settings import settings
from lifetrace.util.time_parser import calculate_scheduled_time
from lifetrace.util.time_utils import get_utc_now

logger = get_logger()

router = APIRouter(prefix="/api/floating-capture", tags=["floating-capture"])

# å¸¸é‡å®šä¹‰
MIN_RESPONSE_LENGTH_THRESHOLD = 50  # LLM å“åº”çš„æœ€å°é•¿åº¦é˜ˆå€¼

# LLM å®¢æˆ·ç«¯å•ä¾‹


@lru_cache(maxsize=1)
def get_llm_client() -> LLMClient:
    """è·å– LLM å®¢æˆ·ç«¯å•ä¾‹"""
    return LLMClient()


@router.post("/extract-todos", response_model=FloatingCaptureResponse)
async def extract_todos_from_capture(request: FloatingCaptureRequest) -> FloatingCaptureResponse:
    """
    ä»æ‚¬æµ®çª—æˆªå›¾ä¸­æå–å¾…åŠäº‹é¡¹

    Args:
        request: åŒ…å« base64 ç¼–ç æˆªå›¾çš„è¯·æ±‚

    Returns:
        æå–å’Œåˆ›å»ºçš„å¾…åŠäº‹é¡¹åˆ—è¡¨
    """
    try:
        total_start = time.time()
        logger.info("ğŸš€ å¼€å§‹å¤„ç†æ‚¬æµ®çª—æˆªå›¾è¯·æ±‚...")

        llm_client = get_llm_client()

        if not llm_client.is_available():
            return FloatingCaptureResponse(
                success=False,
                message="LLM æœåŠ¡å½“å‰ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®",
                extracted_todos=[],
                created_todos=[],
                created_count=0,
            )

        # è·å–å·²æœ‰å¾…åŠåˆ—è¡¨ç”¨äºå»é‡
        step_start = time.time()
        existing_todos = todo_mgr.list_todos(limit=1000, status="active")
        existing_todos += todo_mgr.list_todos(limit=1000, status="draft")
        logger.info(
            f"â±ï¸ è·å–å·²æœ‰å¾…åŠåˆ—è¡¨: {time.time() - step_start:.3f}s (å…± {len(existing_todos)} æ¡)"
        )

        # è°ƒç”¨è§†è§‰æ¨¡å‹æå–å¾…åŠ
        step_start = time.time()
        extracted_todos = _call_vision_model_with_base64(
            llm_client=llm_client,
            image_base64=request.image_base64,
            existing_todos=existing_todos,
        )
        vision_time = time.time() - step_start
        logger.info(f"â±ï¸ è§†è§‰æ¨¡å‹è°ƒç”¨æ€»è€—æ—¶: {vision_time:.3f}s")

        if not extracted_todos:
            total_time = time.time() - total_start
            logger.info(f"âœ… æ‚¬æµ®çª—æˆªå›¾å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s (æœªæ£€æµ‹åˆ°å¾…åŠäº‹é¡¹)")
            return FloatingCaptureResponse(
                success=True,
                message="æˆªå›¾ä¸­æœªæ£€æµ‹åˆ°å¾…åŠäº‹é¡¹",
                extracted_todos=[],
                created_todos=[],
                created_count=0,
            )

        # è½¬æ¢ä¸º ExtractedTodo åˆ—è¡¨ï¼ˆä¸è®¡å…¥æ ¸å¿ƒå¤„ç†æ—¶é—´ï¼‰
        conversion_start = time.time()
        extracted_todo_models = [
            ExtractedTodo(
                title=todo.get("title", ""),
                description=todo.get("description"),
                time_info=todo.get("time_info"),
                source_text=todo.get("source_text"),
                confidence=todo.get("confidence", 0.5),
            )
            for todo in extracted_todos
        ]
        conversion_time = time.time() - conversion_start
        logger.info(f"â±ï¸ æ•°æ®è½¬æ¢è€—æ—¶: {conversion_time:.3f}s")

        # å¦‚æœéœ€è¦åˆ›å»ºå¾…åŠ
        created_todos: list[CreatedTodo] = []
        created_count = 0

        if request.create_todos:
            step_start = time.time()
            for todo_data in extracted_todos:
                try:
                    result = _create_draft_todo(todo_data)
                    if result:
                        created_count += 1
                        created_todos.append(
                            CreatedTodo(
                                id=result["id"],
                                name=result["name"],
                                scheduled_time=result.get("scheduled_time"),
                            )
                        )
                except Exception as e:
                    logger.error(f"åˆ›å»ºå¾…åŠå¤±è´¥: {e}", exc_info=True)
                    continue
            create_time = time.time() - step_start
            logger.info(f"â±ï¸ åˆ›å»ºå¾…åŠåˆ°æ•°æ®åº“: {create_time:.3f}s")

        total_time = time.time() - total_start
        logger.info(
            f"âœ… æ‚¬æµ®çª—æˆªå›¾å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s (æå– {len(extracted_todos)} ä¸ªå¾…åŠï¼Œåˆ›å»º {created_count} ä¸ª)"
        )

        return FloatingCaptureResponse(
            success=True,
            message=f"æˆåŠŸæå– {len(extracted_todos)} ä¸ªå¾…åŠï¼Œåˆ›å»º {created_count} ä¸ª",
            extracted_todos=extracted_todo_models,
            created_todos=created_todos,
            created_count=created_count,
        )

    except Exception as e:
        logger.error(f"å¤„ç†æ‚¬æµ®çª—æˆªå›¾å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†æˆªå›¾å¤±è´¥: {e!s}") from e


def _process_llm_response(response: Any, api_time: float) -> str | None:
    """
    å¤„ç† LLM API å“åº”ï¼Œæå–å“åº”æ–‡æœ¬

    Args:
        response: LLM API å“åº”å¯¹è±¡
        api_time: API è°ƒç”¨è€—æ—¶

    Returns:
        å“åº”æ–‡æœ¬ï¼Œå¦‚æœå“åº”æ— æ•ˆåˆ™è¿”å› None
    """
    # æ£€æŸ¥å“åº”ç»“æ„
    if not response or not hasattr(response, "choices") or len(response.choices) == 0:
        logger.error(f"LLM API è¿”å›å¼‚å¸¸å“åº”ç»“æ„: {response}")
        return None

    # æ£€æŸ¥ Token ä½¿ç”¨æƒ…å†µï¼ˆè¯Šæ–­æ€§èƒ½é—®é¢˜ï¼‰
    usage = getattr(response, "usage", None)
    if usage:
        prompt_tokens = getattr(usage, "prompt_tokens", 0)
        completion_tokens = getattr(usage, "completion_tokens", 0)
        total_tokens = getattr(usage, "total_tokens", 0)
        logger.info(
            f"  ğŸ”¢ Token ä½¿ç”¨: prompt={prompt_tokens}, completion={completion_tokens}, total={total_tokens}"
        )
        if completion_tokens > 0:
            tokens_per_second = completion_tokens / api_time if api_time > 0 else 0
            logger.info(f"  âš¡ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† thinking æ¨¡å¼
    choice = response.choices[0]
    message = choice.message

    # æ£€æŸ¥æ˜¯å¦æœ‰ reasoning_contentï¼ˆthinking æ¨¡å¼çš„è¾“å‡ºï¼‰
    reasoning_content = getattr(message, "reasoning_content", None)
    if reasoning_content:
        reasoning_len = len(reasoning_content) if reasoning_content else 0
        logger.warning(f"  ğŸ§  æ£€æµ‹åˆ° Thinking æ¨¡å¼ï¼Œæ¨ç†å†…å®¹é•¿åº¦: {reasoning_len} å­—ç¬¦")

    # æ£€æŸ¥ finish_reason
    finish_reason = getattr(choice, "finish_reason", None)
    if finish_reason:
        logger.info(f"  ğŸ“‹ å“åº”å®ŒæˆåŸå› : {finish_reason}")
        if finish_reason == "length":
            logger.warning("  âš ï¸ å“åº”å› è¾¾åˆ° max_tokens é™åˆ¶è€Œæˆªæ–­ï¼")

    response_text = message.content or ""
    if not response_text:
        logger.warning("è§†è§‰æ¨¡å‹è¿”å›ç©ºå“åº”")
        return None

    logger.info(f"  ğŸ“ LLM å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")

    # è¯Šæ–­ï¼šè®°å½•å“åº”å‰100ä¸ªå­—ç¬¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    preview = response_text[:100].replace("\n", "\\n")
    logger.debug(f"  ğŸ‘€ å“åº”é¢„è§ˆ: {preview}...")

    return response_text


def _call_vision_model_with_base64(
    llm_client: LLMClient,
    image_base64: str,
    existing_todos: list[dict[str, Any]],
) -> list[dict[str, Any]]:
    """
    ä½¿ç”¨ base64 å›¾ç‰‡ç›´æ¥è°ƒç”¨è§†è§‰æ¨¡å‹

    Args:
        llm_client: LLM å®¢æˆ·ç«¯
        image_base64: Base64 ç¼–ç çš„å›¾ç‰‡
        existing_todos: å·²æœ‰å¾…åŠåˆ—è¡¨

    Returns:
        æå–çš„å¾…åŠåˆ—è¡¨
    """
    try:
        step_start = time.time()

        # æ ¼å¼åŒ–å·²æœ‰å¾…åŠåˆ—è¡¨ä¸º JSON
        existing_todos_json = json.dumps(
            [
                {
                    "id": todo.get("id"),
                    "name": todo.get("name"),
                    "description": todo.get("description"),
                }
                for todo in existing_todos[:50]  # é™åˆ¶æ•°é‡
            ],
            ensure_ascii=False,
            indent=2,
        )

        # ä»é…ç½®æ–‡ä»¶åŠ è½½æç¤ºè¯
        system_prompt = get_prompt("auto_todo_detection", "system_assistant")
        user_prompt = get_prompt(
            "auto_todo_detection",
            "user_prompt",
            existing_todos_json=existing_todos_json,
        )

        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = f"{system_prompt}\n\n{user_prompt}"

        # ç¡®ä¿ base64 æœ‰æ­£ç¡®çš„å‰ç¼€
        if not image_base64.startswith("data:"):
            image_base64 = f"data:image/png;base64,{image_base64}"

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = [
            {
                "type": "image_url",
                "image_url": {"url": image_base64},
            },
            {"type": "text", "text": full_prompt},
        ]

        messages = cast("list[ChatCompletionMessageParam]", [{"role": "user", "content": content}])

        prep_time = time.time() - step_start
        logger.info(f"  â±ï¸ æ„å»ºè¯·æ±‚å‡†å¤‡: {prep_time:.3f}s")

        # è·å–è§†è§‰æ¨¡å‹é…ç½®
        vision_model = settings.llm.vision_model or settings.llm.model

        # è®¡ç®—å›¾ç‰‡å¤§å°
        image_size_kb = len(image_base64) * 3 / 4 / 1024  # Base64 è§£ç åå¤§å°ä¼°ç®—
        logger.info(f"ğŸ“· è°ƒç”¨è§†è§‰æ¨¡å‹ {vision_model} (å›¾ç‰‡å¤§å°: {image_size_kb:.1f}KB)")

        # è°ƒç”¨æ¨¡å‹
        api_start = time.time()
        try:
            client = llm_client._get_client()
            response = client.chat.completions.create(
                model=vision_model,
                messages=messages,
                temperature=0.3,
                max_tokens=2000,
                timeout=60,
                extra_body={"enable_thinking": False},  # æ˜¾å¼ç¦ç”¨ thinking æ¨¡å¼
            )
        except Exception as api_error:
            logger.error(f"LLM API è°ƒç”¨å¤±è´¥: {api_error}", exc_info=True)
            raise

        api_time = time.time() - api_start
        logger.info(f"  â±ï¸ LLM API è°ƒç”¨è€—æ—¶: {api_time:.3f}s")

        # å¤„ç†å“åº”
        response_text = _process_llm_response(response, api_time)
        if not response_text:
            return []

        # è§£æå“åº”
        parse_start = time.time()
        result = _parse_llm_response(response_text)
        logger.info(f"  â±ï¸ è§£æå“åº”: {time.time() - parse_start:.3f}s (æå–åˆ° {len(result)} ä¸ªå¾…åŠ)")

        if not result and len(response_text) < MIN_RESPONSE_LENGTH_THRESHOLD:
            logger.warning(f"LLM å“åº”å¼‚å¸¸çŸ­ï¼ˆ{len(response_text)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯é”™è¯¯æ¶ˆæ¯æˆ–æ ¼å¼é—®é¢˜")

        return result

    except Exception as e:
        logger.error(f"è°ƒç”¨è§†è§‰æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
        return []


def _parse_llm_response(response_text: str) -> list[dict[str, Any]]:
    """
    è§£æ LLM å“åº”

    Args:
        response_text: LLM è¿”å›çš„æ–‡æœ¬

    Returns:
        å¾…åŠåˆ—è¡¨
    """

    def _extract_todos_from_result(result: dict[str, Any]) -> list[dict[str, Any]]:
        """ä»ç»“æœä¸­æå–å¾…åŠåˆ—è¡¨"""
        if "new_todos" in result:
            return result["new_todos"]
        if "todos" in result:
            return result["todos"]
        return []

    try:
        # å°è¯•æå– JSON
        json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            todos = _extract_todos_from_result(result)
            if todos:
                return todos

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•ç›´æ¥è§£æ
        result = json.loads(response_text)
        todos = _extract_todos_from_result(result)
        if todos:
            return todos

        logger.warning("LLM å“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œæœªæ‰¾åˆ° new_todos æˆ– todos å­—æ®µ")
        return []

    except json.JSONDecodeError as e:
        logger.error(f"è§£æ LLM å“åº” JSON å¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"è§£æ LLM å“åº”å¤±è´¥: {e}", exc_info=True)
        return []


def _create_draft_todo(todo_data: dict[str, Any]) -> dict[str, Any] | None:
    """
    åˆ›å»º draft çŠ¶æ€çš„å¾…åŠ

    Args:
        todo_data: å¾…åŠæ•°æ®

    Returns:
        åˆ›å»ºç»“æœæˆ– None
    """
    title = todo_data.get("title", "").strip()
    if not title:
        return None

    description = todo_data.get("description")
    if description:
        description = description.strip()

    source_text = todo_data.get("source_text", "")
    time_info = todo_data.get("time_info", {})
    confidence = todo_data.get("confidence")

    # è®¡ç®— scheduled_time
    scheduled_time = None
    if time_info:
        try:
            reference_time = get_utc_now()
            scheduled_time = calculate_scheduled_time(time_info, reference_time)
        except Exception as e:
            logger.warning(f"è®¡ç®— scheduled_time å¤±è´¥: {e}")

    # æ„å»º user_notes
    user_notes_parts = ["æ¥æº: æ‚¬æµ®çª—æˆªå›¾"]
    if source_text:
        user_notes_parts.append(f"æ¥æºæ–‡æœ¬: {source_text}")
    if time_info and time_info.get("raw_text"):
        user_notes_parts.append(f"æ—¶é—´: {time_info.get('raw_text')}")
    if confidence is not None:
        user_notes_parts.append(f"ç½®ä¿¡åº¦: {confidence:.0%}")
    user_notes = "\n".join(user_notes_parts)

    # åˆ›å»ºå¾…åŠ
    todo_id = todo_mgr.create_todo(
        name=title,
        description=description,
        user_notes=user_notes,
        start_time=scheduled_time,
        status="draft",
        priority="none",
        tags=["æ‚¬æµ®çª—æå–"],
    )

    if todo_id:
        logger.info(f"åˆ›å»º draft å¾…åŠ: {todo_id} - {title}")
        return {
            "id": todo_id,
            "name": title,
            "scheduled_time": scheduled_time.isoformat() if scheduled_time else None,
        }

    return None


@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    llm_client = get_llm_client()
    return {
        "status": "ok",
        "llm_available": llm_client.is_available(),
    }
