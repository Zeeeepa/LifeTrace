/**
 * 全局音频录音状态管理
 *
 * 将录音状态和资源提升到全局层面，使录音在面板切换时不会中断。
 * 核心思路：
 * - 使用模块级变量存储不可序列化的资源（WebSocket、AudioContext、MediaStream）
 * - 使用 Zustand store 存储可序列化的状态（isRecording、transcriptionText 等）
 * - 组件卸载时不清理录音资源，只有显式调用 stopRecording 才会停止
 */

import { create } from "zustand";

// ========== 类型定义 ==========

interface TodoItem {
	title: string;
	description?: string;
	deadline?: string;
	source_text?: string;
}

interface ScheduleItem {
	title: string;
	time?: string;
	description?: string;
	source_text?: string;
}

type TranscriptionCallback = (text: string, isFinal: boolean) => void

type RealtimeNlpCallback = (data: {
		optimizedText?: string;
		todos?: TodoItem[];
		schedules?: ScheduleItem[];
	}) => void

type ErrorCallback = (error: Error) => void

interface AudioRecordingState {
	/** 是否正在录音 */
	isRecording: boolean;
	/** 录音开始时间（毫秒时间戳） */
	recordingStartedAt: number | null;
	/** 录音开始的 Date 对象（用于时间标签） */
	recordingStartedDate: Date | null;
	/** 上一个 final 文本的时间戳（用于计算段落时间） */
	lastFinalEndMs: number | null;

	// ===== 转录数据（在面板切换时保持） =====
	/** 原始转录文本 */
	transcriptionText: string;
	/** 正在识别的部分文本（未确认） */
	partialText: string;
	/** 优化后的文本 */
	optimizedText: string;
	/** 段落时间（秒） */
	segmentTimesSec: number[];
	/** 段落时间标签 */
	segmentTimeLabels: string[];
	/** 段落录音 ID */
	segmentRecordingIds: number[];
	/** 段落偏移（秒） */
	segmentOffsetsSec: number[];
	/** 实时提取的待办 */
	liveTodos: TodoItem[];
	/** 实时提取的日程 */
	liveSchedules: ScheduleItem[];
}

interface AudioRecordingActions {
	/** 开始录音 */
	startRecording: (
		onTranscription: TranscriptionCallback,
		onRealtimeNlp?: RealtimeNlpCallback,
		onError?: ErrorCallback,
		is24x7?: boolean,
	) => Promise<void>;
	/** 停止录音 */
	stopRecording: (segmentTimestamps?: number[]) => void;
	/** 重置时间戳引用（用于新段落） */
	resetLastFinalEnd: () => void;
	/** 更新 lastFinalEndMs */
	updateLastFinalEnd: (ms: number) => void;

	// ===== 转录数据更新方法 =====
	/** 追加转录文本 */
	appendTranscriptionText: (text: string) => void;
	/** 设置部分文本 */
	setPartialText: (text: string) => void;
	/** 设置优化文本 */
	setOptimizedText: (text: string) => void;
	/** 追加段落数据 */
	appendSegmentData: (data: {
		timeSec: number;
		timeLabel: string;
		recordingId: number;
		offsetSec: number;
	}) => void;
	/** 设置实时待办 */
	setLiveTodos: (todos: TodoItem[]) => void;
	/** 设置实时日程 */
	setLiveSchedules: (schedules: ScheduleItem[]) => void;
	/** 清空录音会话数据（开始新录音时调用） */
	clearSessionData: () => void;
}

type AudioRecordingStore = AudioRecordingState & AudioRecordingActions;

// ========== 模块级资源存储（不可序列化） ==========

let wsRef: WebSocket | null = null;
let audioContextRef: AudioContext | null = null;
let processorRef: ScriptProcessorNode | null = null;
let mediaStreamRef: MediaStream | null = null;

// 回调函数引用（用于在 WebSocket 消息中调用）
let currentOnTranscription: TranscriptionCallback | null = null;
let currentOnRealtimeNlp: RealtimeNlpCallback | null = null;
let currentOnError: ErrorCallback | null = null;

// ========== 内部辅助函数 ==========

/**
 * 获取 API 基础 URL
 */
function getApiBaseUrl(): string {
	return (
		process.env.NEXT_PUBLIC_API_URL ||
		(typeof window !== "undefined" &&
			(window as Window & { __BACKEND_URL__?: string }).__BACKEND_URL__) ||
		"http://localhost:8100"
	);
}

/**
 * 清理录音资源
 */
function cleanupRecordingResources(segmentTimestamps?: number[]): void {
	// 停止 WebAudio
	if (processorRef) {
		try {
			processorRef.disconnect();
		} catch {
			// ignore
		}
		processorRef.onaudioprocess = null;
		processorRef = null;
	}
	if (audioContextRef) {
		try {
			audioContextRef.close();
		} catch {
			// ignore
		}
		audioContextRef = null;
	}
	if (mediaStreamRef) {
		for (const track of mediaStreamRef.getTracks()) {
			track.stop();
		}
		mediaStreamRef = null;
	}
	if (wsRef) {
		// 发送停止消息，包含时间戳数组（如果提供）
		const stopMessage: { type: string; segment_timestamps?: number[] } = {
			type: "stop",
		};
		if (segmentTimestamps && segmentTimestamps.length > 0) {
			stopMessage.segment_timestamps = segmentTimestamps;
		}
		try {
			wsRef.send(JSON.stringify(stopMessage));
			wsRef.close();
		} catch {
			// ignore
		}
		wsRef = null;
	}

	// 清理回调引用
	currentOnTranscription = null;
	currentOnRealtimeNlp = null;
	currentOnError = null;
}

// ========== Zustand Store ==========

export const useAudioRecordingStore = create<AudioRecordingStore>((set, get) => ({
	// ===== 核心状态 =====
	isRecording: false,
	recordingStartedAt: null,
	recordingStartedDate: null,
	lastFinalEndMs: null,

	// ===== 转录数据 =====
	transcriptionText: "",
	partialText: "",
	optimizedText: "",
	segmentTimesSec: [],
	segmentTimeLabels: [],
	segmentRecordingIds: [],
	segmentOffsetsSec: [],
	liveTodos: [],
	liveSchedules: [],

	// ===== Actions =====

	startRecording: async (onTranscription, onRealtimeNlp, onError, is24x7 = false) => {
		// 如果已经在录音，直接返回
		if (get().isRecording) {
			console.warn("[AudioRecordingStore] Already recording, ignoring start request");
			return;
		}

		try {
			// 获取麦克风权限
			const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
			mediaStreamRef = stream;

			// 保存回调引用
			currentOnTranscription = onTranscription;
			currentOnRealtimeNlp = onRealtimeNlp || null;
			currentOnError = onError || null;

			// 连接到后端 WebSocket
			const apiBaseUrl = getApiBaseUrl();
			const wsUrl = apiBaseUrl.replace("http://", "ws://").replace("https://", "wss://");
			const wsEndpoint = `${wsUrl}/api/audio/transcribe`;
			const ws = new WebSocket(wsEndpoint);
			ws.binaryType = "arraybuffer";

			ws.onopen = () => {
				// 发送初始化消息
				ws.send(JSON.stringify({ is_24x7: is24x7 }));

				// 使用 WebAudio 直接发送 PCM16(16k) 到后端
				type AudioContextCtor = typeof AudioContext & {
					webkitAudioContext?: typeof AudioContext;
				};
				const AudioCtx = (window.AudioContext ||
					(window as unknown as { webkitAudioContext?: typeof AudioContext })
						.webkitAudioContext) as AudioContextCtor;
				const audioContext = new AudioCtx({ sampleRate: 16000 });
				audioContextRef = audioContext;

				const source = audioContext.createMediaStreamSource(stream);
				const processor = audioContext.createScriptProcessor(4096, 1, 1);
				processorRef = processor;

				processor.onaudioprocess = (e) => {
					if (ws.readyState !== WebSocket.OPEN) return;
					const input = e.inputBuffer.getChannelData(0); // Float32 [-1, 1]
					// 转 Int16 little-endian
					const buffer = new ArrayBuffer(input.length * 2);
					const view = new DataView(buffer);
					for (let i = 0; i < input.length; i++) {
						const s = Math.max(-1, Math.min(1, input[i]));
						view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true);
					}
					ws.send(buffer);
				};

				source.connect(processor);
				processor.connect(audioContext.destination);

				// 记录开始时间并更新状态
				const now = Date.now();
				set({
					isRecording: true,
					recordingStartedAt: now,
					recordingStartedDate: new Date(),
					lastFinalEndMs: null,
				});
			};

			ws.onmessage = (event) => {
				try {
					if (typeof event.data === "string") {
						const data = JSON.parse(event.data);

						// 转录结果
						if (data.header?.name === "TranscriptionResultChanged") {
							const text = data.payload?.result;
							const isFinal = data.payload?.is_final || false;
							if (text && currentOnTranscription) {
								currentOnTranscription(text, isFinal);
							}
							return;
						}

						// 实时优化文本
						if (data.header?.name === "OptimizedTextChanged") {
							const text = data.payload?.text;
							if (currentOnRealtimeNlp && typeof text === "string") {
								currentOnRealtimeNlp({ optimizedText: text });
							}
							return;
						}

						// 实时提取结果
						if (data.header?.name === "ExtractionChanged") {
							const todos = data.payload?.todos;
							const schedules = data.payload?.schedules;
							if (currentOnRealtimeNlp) {
								currentOnRealtimeNlp({
									todos: Array.isArray(todos) ? todos : [],
									schedules: Array.isArray(schedules) ? schedules : [],
								});
							}
							return;
						}
					}
				} catch (error) {
					console.error("Failed to parse transcription data:", error);
				}
			};

			ws.onerror = (error) => {
				const errorMessage =
					error instanceof Error
						? error.message
						: "WebSocket连接错误，请检查后端服务是否运行";
				console.error("WebSocket error:", errorMessage, error);
				set({ isRecording: false });
				if (currentOnError) {
					currentOnError(new Error(errorMessage));
				}
			};

			ws.onclose = (event) => {
				set({
					isRecording: false,
					recordingStartedAt: null,
					recordingStartedDate: null,
					lastFinalEndMs: null,
				});

				// 正常关闭不触发错误
				if (event.wasClean) {
					return;
				}

				// 异常关闭提供详细错误信息
				let errorMessage = "WebSocket连接异常关闭";
				switch (event.code) {
					case 1006:
						errorMessage =
							"WebSocket连接异常断开，可能是网络问题或服务器未响应。请检查：\n1. 后端服务是否正常运行\n2. 网络连接是否正常\n3. 防火墙或代理设置是否正确";
						break;
					case 1000:
						return;
					case 1001:
						errorMessage = "服务器主动断开连接（端点离开）";
						break;
					case 1011:
						errorMessage = "服务器内部错误导致连接关闭";
						break;
					default:
						errorMessage = `WebSocket连接异常关闭: ${event.reason || `错误代码 ${event.code}`}`;
				}

				console.error("WebSocket closed abnormally:", {
					code: event.code,
					reason: event.reason,
					wasClean: event.wasClean,
				});

				if (currentOnError) {
					currentOnError(new Error(errorMessage));
				}
			};

			wsRef = ws;
		} catch (error) {
			console.error("Failed to start recording:", error);
			if (onError) {
				onError(error as Error);
			}
		}
	},

	stopRecording: (segmentTimestamps) => {
		cleanupRecordingResources(segmentTimestamps);
		set({
			isRecording: false,
			recordingStartedAt: null,
			recordingStartedDate: null,
			lastFinalEndMs: null,
		});
	},

	resetLastFinalEnd: () => {
		set({ lastFinalEndMs: null });
	},

	updateLastFinalEnd: (ms) => {
		set({ lastFinalEndMs: ms });
	},

	// ===== 转录数据更新方法 =====

	appendTranscriptionText: (text) => {
		set((state) => {
			const prev = state.transcriptionText;
			const needsGap = prev && !prev.endsWith("\n");
			return {
				transcriptionText: `${prev}${needsGap ? "\n" : ""}${text}\n`,
			};
		});
	},

	setPartialText: (text) => {
		set({ partialText: text });
	},

	setOptimizedText: (text) => {
		set({ optimizedText: text });
	},

	appendSegmentData: (data) => {
		set((state) => ({
			segmentTimesSec: [...state.segmentTimesSec, data.timeSec],
			segmentTimeLabels: [...state.segmentTimeLabels, data.timeLabel],
			segmentRecordingIds: [...state.segmentRecordingIds, data.recordingId],
			segmentOffsetsSec: [...state.segmentOffsetsSec, data.offsetSec],
		}));
	},

	setLiveTodos: (todos) => {
		set({ liveTodos: todos });
	},

	setLiveSchedules: (schedules) => {
		set({ liveSchedules: schedules });
	},

	clearSessionData: () => {
		set({
			transcriptionText: "",
			partialText: "",
			optimizedText: "",
			segmentTimesSec: [],
			segmentTimeLabels: [],
			segmentRecordingIds: [],
			segmentOffsetsSec: [],
			liveTodos: [],
			liveSchedules: [],
		});
	},
}));

// ========== 辅助 Hooks ==========

/**
 * 获取录音开始后的经过时间（毫秒）
 */
export function getRecordingElapsedMs(): number {
	const { recordingStartedAt } = useAudioRecordingStore.getState();
	if (!recordingStartedAt) return 0;
	return Date.now() - recordingStartedAt;
}

/**
 * 获取段落的开始时间（相对于录音开始）
 * 优先使用 lastFinalEndMs，否则使用录音开始时间
 */
export function getSegmentStartMs(): number {
	const { recordingStartedAt, lastFinalEndMs } = useAudioRecordingStore.getState();
	if (!recordingStartedAt) return 0;
	return lastFinalEndMs ?? recordingStartedAt;
}
