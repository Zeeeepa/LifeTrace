"""聊天核心路由：基础问答与流式聊天。"""

from datetime import datetime

from fastapi import Depends, HTTPException
from fastapi.responses import StreamingResponse

from lifetrace.core.dependencies import get_chat_service, get_rag_service
from lifetrace.schemas.chat import ChatMessage, ChatResponse
from lifetrace.services.chat_service import ChatService
from lifetrace.services.dify_client import call_dify_chat

from .base import _create_llm_stream_generator, logger, router


@router.post("", response_model=ChatResponse)
async def chat_with_llm(
    message: ChatMessage,
    chat_service: ChatService = Depends(get_chat_service),
):
    """与LLM聊天接口 - 集成RAG功能"""

    try:
        logger.info(f"收到聊天消息: {message.message}")

        # 使用RAG服务处理查询
        rag_service = get_rag_service()
        rag_result = await rag_service.process_query(message.message)

        if rag_result.get("success", False):
            success = True  # noqa: F841
            response = ChatResponse(
                response=rag_result["response"],
                timestamp=datetime.now(),
                query_info=rag_result.get("query_info"),
                retrieval_info=rag_result.get("retrieval_info"),
                performance=rag_result.get("performance"),
            )

            return response
        else:
            # 如果RAG处理失败，返回错误信息
            error_msg = rag_result.get("response", "处理您的查询时出现了错误，请稍后重试。")

            return ChatResponse(
                response=error_msg,
                timestamp=datetime.now(),
                query_info={
                    "original_query": message.message,
                    "error": rag_result.get("error"),
                },
            )

    except Exception as e:
        logger.error(f"聊天处理失败: {e}")

        return ChatResponse(
            response="抱歉，系统暂时无法处理您的请求，请稍后重试。",
            timestamp=datetime.now(),
            query_info={"original_query": message.message, "error": str(e)},
        )


@router.post("/stream")
async def chat_with_llm_stream(
    message: ChatMessage,
    chat_service: ChatService = Depends(get_chat_service),
):
    """与LLM聊天接口（流式输出）

    支持额外的 mode 字段：
    - 默认为现有行为（走本地 LLM + RAG）
    - 当 mode == \"dify_test\" 时，走 Dify 测试通道
    """
    try:
        logger.info(
            f"[stream] 收到聊天消息: {message.message}, project_id: {message.project_id}, task_ids: {message.task_ids}"
        )

        # 1. 会话初始化与聊天会话创建
        session_id = _ensure_stream_session(message, chat_service)

        # 2. Dify 测试模式（直接返回）
        if getattr(message, "mode", None) == "dify_test":
            return _create_dify_streaming_response(message, chat_service, session_id)

        # 3. 根据 use_rag 构建 messages / temperature，并处理 RAG 失败场景
        (
            messages,
            temperature,
            user_message_to_save,
            error_response,
        ) = await _build_stream_messages_and_temperature(message, session_id)

        if error_response is not None:
            return error_response

        # 4. 保存用户原始输入（不含 system prompt）
        chat_service.add_message(
            session_id=session_id,
            role="user",
            content=user_message_to_save,
        )

        # 5. 调用 LLM，生成统一的流式响应
        rag_svc = get_rag_service()
        token_generator = _create_llm_stream_generator(
            rag_svc=rag_svc,
            messages=messages,
            temperature=temperature,
            chat_service=chat_service,
            meta={
                "session_id": session_id,
                "endpoint": "stream_chat",
                "feature_type": "project_assistant" if message.project_id else "event_assistant",
                "user_query": message.message,
                "additional_info": {
                    "project_id": message.project_id,
                    "task_ids": message.task_ids,
                    "selected_tasks_count": len(message.task_ids) if message.task_ids else 0,
                },
            },
        )

        headers = {
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "X-Session-Id": session_id,  # 返回 session_id 供前端使用
        }
        return StreamingResponse(
            token_generator, media_type="text/plain; charset=utf-8", headers=headers
        )

    except Exception as e:  # noqa: BLE001
        logger.error(f"[stream] 聊天处理失败: {e}")
        raise HTTPException(status_code=500, detail="流式聊天处理失败") from e


def _ensure_stream_session(message: ChatMessage, chat_service: ChatService) -> str:
    """确保流式聊天有有效的 session，并在需要时创建数据库会话。"""
    session_id = message.conversation_id or chat_service.generate_session_id()
    if not message.conversation_id:
        logger.info(f"[stream] 创建新会话: {session_id}")

    chat = chat_service.get_chat_by_session_id(session_id)
    if not chat:
        chat_type = "project" if message.project_id else "event"
        title = message.message[:50] if len(message.message) > 50 else message.message  # noqa: PLR2004
        chat_service.create_chat(
            session_id=session_id,
            chat_type=chat_type,
            title=title,
            context_id=message.project_id or None,
        )
        logger.info(f"[stream] 在数据库中创建会话: {session_id}, 类型: {chat_type}")

    return session_id


def _create_dify_streaming_response(
    message: ChatMessage,
    chat_service: ChatService,
    session_id: str,
) -> StreamingResponse:
    """处理 Dify 测试模式，保持一次性 blocking 调用但以流式接口返回。"""
    logger.info("[stream] 进入 Dify 测试模式")

    # 保存用户消息
    chat_service.add_message(
        session_id=session_id,
        role="user",
        content=message.message,
    )

    def dify_token_generator():
        try:
            result = call_dify_chat(message.message)
            yield result

            # 保存助手回复
            chat_service.add_message(
                session_id=session_id,
                role="assistant",
                content=result,
            )
            logger.info("[stream][dify] 消息已保存到数据库")
        except Exception as e:  # noqa: BLE001
            logger.error(f"[stream][dify] 生成失败: {e}")
            yield "Dify 测试模式调用失败，请检查后端 Dify 配置。"

    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",
        "X-Session-Id": session_id,
    }
    return StreamingResponse(
        dify_token_generator(), media_type="text/plain; charset=utf-8", headers=headers
    )


async def _build_stream_messages_and_temperature(
    message: ChatMessage,
    session_id: str,
) -> tuple[list[dict[str, str]], float, str, StreamingResponse | None]:
    """根据 use_rag / 前端 prompt 构建 messages 与 temperature。

    返回 (messages, temperature, user_message_to_save, error_response)。
    当 RAG 失败时，error_response 不为 None，调用方应直接返回该响应。
    """
    user_message_to_save = message.message

    if message.use_rag:
        # 使用 RAG 服务的流式处理方法，避免重复的意图识别
        rag_service = get_rag_service()
        rag_result = await rag_service.process_query_stream(
            message.message,
            message.project_id,
            message.task_ids,
            session_id,
        )

        if not rag_result.get("success", False):
            error_msg = rag_result.get(
                "response",
                "处理您的查询时出现了错误，请稍后重试。",
            )

            async def error_generator():
                yield error_msg

            return (
                [],
                0.7,
                user_message_to_save,
                StreamingResponse(
                    error_generator(),
                    media_type="text/plain; charset=utf-8",
                ),
            )

        messages = rag_result.get("messages", [])
        temperature = rag_result.get("temperature", 0.7)
        return messages, temperature, user_message_to_save, None

    # 不使用 RAG，直接构建消息，兼容前端已构建好的 system prompt
    full_message = message.message
    if "用户输入:" in full_message or "User input:" in full_message:
        if "用户输入:" in full_message:
            parts = full_message.split("用户输入:", 1)
        else:
            parts = full_message.split("User input:", 1)

        if len(parts) == 2:  # noqa: PLR2004
            system_prompt = parts[0].strip()
            user_input = parts[1].strip()
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input},
            ]
            user_message_to_save = user_input
        else:
            messages = [{"role": "user", "content": full_message}]
    else:
        messages = [{"role": "user", "content": full_message}]

    return messages, 0.7, user_message_to_save, None
