import asyncio
from collections.abc import Generator
from datetime import datetime
from typing import Any

from lifetrace.llm.context_builder import ContextBuilder
from lifetrace.llm.llm_client import LLMClient
from lifetrace.llm.retrieval_service import RetrievalService
from lifetrace.storage import chat_mgr, project_mgr, task_mgr
from lifetrace.util.logging_config import get_logger
from lifetrace.util.prompt_loader import get_prompt
from lifetrace.util.query_parser import QueryConditions, QueryParser
from lifetrace.util.settings import settings
from lifetrace.util.time_utils import get_utc_now

logger = get_logger()


class RAGService:
    """RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æœåŠ¡ï¼Œæ•´åˆæŸ¥è¯¢è§£æã€æ•°æ®æ£€ç´¢ã€ä¸Šä¸‹æ–‡æ„å»ºå’ŒLLMç”Ÿæˆ"""

    def __init__(self):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        """
        self.llm_client = LLMClient()
        self.retrieval_service = RetrievalService()
        self.context_builder = ContextBuilder()
        self.query_parser = QueryParser(self.llm_client)

        logger.info("RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def _handle_direct_query(
        self, user_query: str, intent_result: dict, start_time: datetime
    ) -> dict[str, Any]:
        """å¤„ç†ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„ç›´æ¥å›å¤"""
        logger.info(f"ç”¨æˆ·æ„å›¾ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢: {intent_result['intent_type']}")
        if self.llm_client.is_available():
            response_text = self._generate_direct_response(user_query, intent_result)
        else:
            response_text = self._fallback_direct_response(user_query, intent_result)

        processing_time = (get_utc_now() - start_time).total_seconds()
        return {
            "success": True,
            "response": response_text,
            "query_info": {
                "original_query": user_query,
                "intent_classification": intent_result,
                "requires_database": False,
            },
            "performance": {
                "processing_time_seconds": processing_time,
                "timestamp": start_time.isoformat(),
            },
        }

    def _get_statistics_if_needed(
        self, query_type: str, user_query: str, parsed_query
    ) -> dict | None:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if query_type != "statistics" and "ç»Ÿè®¡" not in user_query:
            return None

        if isinstance(parsed_query, QueryConditions):
            conditions = parsed_query
        else:
            conditions = QueryConditions(
                start_date=parsed_query.get("start_date"),
                end_date=parsed_query.get("end_date"),
                app_names=parsed_query.get("app_names", []),
                keywords=parsed_query.get("keywords", []),
            )
        return self.retrieval_service.get_statistics(conditions)

    def _build_context_for_query(
        self, query_type: str, user_query: str, retrieved_data: list, stats: dict | None
    ) -> str:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹æ„å»ºä¸Šä¸‹æ–‡"""
        logger.info("å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡")
        if query_type == "statistics":
            return self.context_builder.build_statistics_context(user_query, retrieved_data, stats)
        if query_type == "search":
            return self.context_builder.build_search_context(user_query, retrieved_data)
        return self.context_builder.build_summary_context(user_query, retrieved_data)

    async def process_query(self, user_query: str, max_results: int = 50) -> dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´RAGæµæ°´çº¿"""
        start_time = get_utc_now()

        try:
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            intent_result = self.llm_client.classify_intent(user_query)

            # ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢æ—¶ç›´æ¥è¿”å›
            if not intent_result.get("needs_database", True):
                return self._handle_direct_query(user_query, intent_result, start_time)

            # æŸ¥è¯¢è§£æå’Œæ£€ç´¢
            logger.info("éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼Œå¼€å§‹æŸ¥è¯¢è§£æ")
            parsed_query = self.query_parser.parse_query(user_query)
            query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"

            logger.info("å¼€å§‹æ•°æ®æ£€ç´¢")
            retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, max_results)

            # è·å–ç»Ÿè®¡å’Œæ„å»ºä¸Šä¸‹æ–‡
            stats = self._get_statistics_if_needed(query_type, user_query, parsed_query)
            context_text = self._build_context_for_query(
                query_type, user_query, retrieved_data, stats
            )

            # LLMç”Ÿæˆ
            logger.info("å¼€å§‹LLMç”Ÿæˆ")
            if self.llm_client.is_available():
                response_text = self.llm_client.generate_summary(user_query, retrieved_data)
            else:
                response_text = self._fallback_response(user_query, retrieved_data, stats)

            processing_time = (get_utc_now() - start_time).total_seconds()
            logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’")

            return {
                "success": True,
                "response": response_text,
                "query_info": {
                    "original_query": user_query,
                    "intent_classification": intent_result,
                    "parsed_query": parsed_query,
                    "query_type": query_type,
                    "requires_database": True,
                },
                "retrieval_info": {
                    "total_found": len(retrieved_data),
                    "data_summary": self._summarize_retrieved_data(retrieved_data),
                },
                "context_info": {
                    "context_length": len(context_text),
                    "llm_available": self.llm_client.is_available(),
                },
                "performance": {
                    "processing_time_seconds": processing_time,
                    "timestamp": start_time.isoformat(),
                },
                "statistics": stats,
            }

        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°äº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚",
                "query_info": {"original_query": user_query},
                "performance": {
                    "processing_time_seconds": (datetime.now() - start_time).total_seconds(),
                    "timestamp": start_time.isoformat(),
                },
            }

    def process_query_sync(self, user_query: str, max_results: int = 50) -> dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æŸ¥è¯¢å¤„ç†

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            max_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡

        Returns:
            åŒ…å«ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        return asyncio.run(self.process_query(user_query, max_results))

    def post_stream_decision(self, user_query: str, output_text: str) -> None:
        """
        æµå¼è¾“å‡ºå®Œæˆåçš„åˆ¤å®š/è®°å½•é’©å­ï¼š
        - ç”¨äºæ‰§è¡Œé‚£äº›â€œå¿…é¡»æ‹¿åˆ°å®Œæ•´è¾“å‡ºæ‰èƒ½åˆ¤æ–­â€çš„é€»è¾‘ï¼ˆä¾‹å¦‚ï¼šæ˜¯å¦éœ€è¦è¿½åŠ å…è´£å£°æ˜ã€æ˜¯å¦è§¦å‘æŸäº›åç»­åŠ¨ä½œç­‰ï¼‰
        - é»˜è®¤å®ç°ä»…åšæ—¥å¿—è®°å½•ï¼Œåç»­å¯æŒ‰éœ€æ‰©å±•
        """
        try:
            if not output_text:
                return
            # ç¤ºä¾‹ï¼šå¦‚æœè¾“å‡ºåŒ…å«ç‰¹å®šæç¤ºè¯ï¼Œåˆ™è®°å½•åˆ°æ—¥å¿—æˆ–è§¦å‘åç»­å¤„ç†
            keywords = ["å…è´£å£°æ˜", "æ•æ„Ÿå†…å®¹", "æ³¨æ„", "æ€»ç»“"]
            if any(kw in output_text for kw in keywords):
                logger.info(
                    f"[post_stream] è¾“å‡ºåŒ…å«å…³é”®æç¤ºï¼Œquery='{user_query[:50]}...' è§¦å‘æ ‡è®°"
                )
            else:
                logger.debug("[post_stream] æ— ç‰¹æ®Šæ ‡è®°")
        except Exception as e:
            logger.debug(f"[post_stream] å¤„ç†å¼‚å¸¸å·²å¿½ç•¥: {e}")

    def stream_query(
        self,
        user_query: str,
        max_results: int = 50,
        temperature_direct: float = 0.7,
        temperature_rag: float = 0.3,
    ) -> Generator[str]:
        """æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œé€token yield æ–‡æœ¬"""
        try:
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            # ä¸éœ€è¦æ•°æ®åº“ï¼šç›´æ¥å¯¹è¯
            if not needs_db:
                yield from self._stream_direct_response(
                    user_query, intent_result, temperature_direct
                )
                return

            # éœ€è¦æ•°æ®åº“ï¼šæ£€ç´¢ + ç”Ÿæˆ
            yield from self._stream_with_retrieval(user_query, max_results, temperature_rag)

        except Exception as e:
            logger.error(f"RAG æµå¼å¤„ç†å¤±è´¥: {e}")
            error_text = "\n[æç¤º] æµå¼å¤„ç†å‡ºç°å¼‚å¸¸ï¼Œå·²ç»“æŸã€‚"
            yield error_text
            try:
                self.post_stream_decision(user_query, error_text)
            except Exception:
                pass

    def _stream_direct_response(
        self, user_query: str, intent_result: dict, temperature: float
    ) -> Generator[str]:
        """æµå¼å¤„ç†ç›´æ¥å¯¹è¯ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰"""
        if not self.llm_client.is_available():
            fallback_text = self._fallback_direct_response(user_query, intent_result)
            yield fallback_text
            self.post_stream_decision(user_query, fallback_text)
            return

        intent_type = intent_result.get("intent_type", "general_chat")
        system_prompt = get_prompt(
            "rag", "system_help" if intent_type == "system_help" else "general_chat"
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query},
        ]

        output_chunks: list[str] = []
        for text in self.llm_client.stream_chat(messages=messages, temperature=temperature):
            if text:
                output_chunks.append(text)
                yield text
        self.post_stream_decision(user_query, "".join(output_chunks))

    def _stream_with_retrieval(
        self, user_query: str, max_results: int, temperature: float
    ) -> Generator[str]:
        """æµå¼å¤„ç†å¸¦æ£€ç´¢çš„æŸ¥è¯¢"""
        parsed_query = self.query_parser.parse_query(user_query)
        query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
        retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, max_results)

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = None
        if query_type == "statistics" or "ç»Ÿè®¡" in user_query:
            try:
                stats = self._get_statistics_if_needed(query_type, user_query, parsed_query)
            except Exception:
                stats = None

        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = self._build_context_for_query(query_type, user_query, retrieved_data, stats)

        # LLM ä¸å¯ç”¨æ—¶è¿”å›å¤‡é€‰
        if not self.llm_client.is_available():
            fallback_text = self._fallback_response(user_query, retrieved_data, stats)
            yield fallback_text
            self.post_stream_decision(user_query, fallback_text)
            return

        # æµå¼ç”Ÿæˆ
        system_prompt = get_prompt("rag", "history_analysis")
        user_prompt = get_prompt(
            "rag", "user_query_template", query=user_query, context=context_text
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        output_chunks: list[str] = []
        for text in self.llm_client.stream_chat(messages=messages, temperature=temperature):
            if text:
                output_chunks.append(text)
                yield text
        self.post_stream_decision(user_query, "".join(output_chunks))

    def get_query_suggestions(self, partial_query: str = "") -> list[str]:
        """
        è·å–æŸ¥è¯¢å»ºè®®

        Args:
            partial_query: éƒ¨åˆ†æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æŸ¥è¯¢å»ºè®®åˆ—è¡¨
        """
        suggestions = [
            "æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©è®°å½•",
            "æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„æ‰€æœ‰è®°å½•",
            "ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨å„åº”ç”¨çš„ä½¿ç”¨æƒ…å†µ",
            "æœç´¢æ˜¨å¤©æµè§ˆå™¨ä¸­çš„å†…å®¹",
            "æ€»ç»“æœ€è¿‘çš„å·¥ä½œç›¸å…³æˆªå›¾",
            "æŸ¥æ‰¾åŒ…å«'é¡¹ç›®'å…³é”®è¯çš„è®°å½•",
            "ç»Ÿè®¡æœ¬æœˆQQèŠå¤©è®°å½•æ•°é‡",
            "æœç´¢æœ€è¿‘3å¤©çš„å­¦ä¹ èµ„æ–™",
            "æ€»ç»“ä¸Šå‘¨çš„ç½‘é¡µæµè§ˆè®°å½•",
            "æŸ¥æ‰¾åŒ…å«'æ–‡æ¡£'çš„æ‰€æœ‰åº”ç”¨è®°å½•",
        ]

        if partial_query:
            # ç®€å•çš„æ¨¡ç³ŠåŒ¹é…
            filtered_suggestions = [
                s for s in suggestions if any(word in s for word in partial_query.split())
            ]
            return filtered_suggestions[:5]

        return suggestions[:5]

    def get_supported_query_types(self) -> dict[str, Any]:
        """
        è·å–æ”¯æŒçš„æŸ¥è¯¢ç±»å‹ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç±»å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "query_types": {
                "summary": {
                    "name": "æ€»ç»“",
                    "description": "å¯¹å†å²è®°å½•è¿›è¡Œæ€»ç»“å’Œæ¦‚æ‹¬",
                    "examples": ["æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©", "æ¦‚æ‹¬æœ€è¿‘çš„å·¥ä½œè®°å½•"],
                },
                "search": {
                    "name": "æœç´¢",
                    "description": "æœç´¢åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®°å½•",
                    "examples": ["æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„è®°å½•", "æœç´¢é¡¹ç›®ç›¸å…³å†…å®¹"],
                },
                "statistics": {
                    "name": "ç»Ÿè®¡",
                    "description": "ç»Ÿè®¡å’Œåˆ†æå†å²è®°å½•æ•°æ®",
                    "examples": ["ç»Ÿè®¡å„åº”ç”¨ä½¿ç”¨æƒ…å†µ", "åˆ†ææœ€è¿‘ä¸€å‘¨çš„æ´»åŠ¨"],
                },
            },
            "supported_apps": [
                "WeChat",
                "QQ",
                "Browser",
                "Chrome",
                "Firefox",
                "Edge",
                "Word",
                "Excel",
                "PowerPoint",
                "Notepad",
                "VSCode",
            ],
            "time_expressions": [
                "ä»Šå¤©",
                "æ˜¨å¤©",
                "æœ€è¿‘3å¤©",
                "æœ¬å‘¨",
                "ä¸Šå‘¨",
                "æœ¬æœˆ",
                "ä¸Šæœˆ",
            ],
        }

    def _summarize_retrieved_data(self, retrieved_data: list[dict[str, Any]]) -> dict[str, Any]:
        """æ€»ç»“æ£€ç´¢åˆ°çš„æ•°æ®"""
        if not retrieved_data:
            return {"apps": {}, "time_range": None, "total": 0}

        app_counts = {}
        timestamps = []

        for record in retrieved_data:
            app_name = record.get("app_name", "æœªçŸ¥åº”ç”¨")
            app_counts[app_name] = app_counts.get(app_name, 0) + 1

            timestamp = record.get("timestamp")
            if timestamp:
                timestamps.append(timestamp)

        time_range = None
        if timestamps:
            timestamps.sort()
            time_range = {"earliest": timestamps[0], "latest": timestamps[-1]}

        return {
            "apps": app_counts,
            "time_range": time_range,
            "total": len(retrieved_data),
        }

    def _fallback_response(
        self,
        user_query: str,
        retrieved_data: list[dict[str, Any]],
        stats: dict[str, Any] = None,
    ) -> str:
        """
        å¤‡ç”¨å“åº”ç”Ÿæˆï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_data: æ£€ç´¢åˆ°çš„æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¤‡ç”¨å“åº”æ–‡æœ¬
        """
        if not retrieved_data:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æŸ¥è¯¢ '{user_query}' ç›¸å…³çš„å†å²è®°å½•ã€‚"

        response_parts = [f"æ ¹æ®æ‚¨çš„æŸ¥è¯¢ '{user_query}'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š", ""]

        # åŸºç¡€ç»Ÿè®¡
        response_parts.append(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(retrieved_data)} æ¡ç›¸å…³è®°å½•")

        # åº”ç”¨åˆ†å¸ƒ
        app_summary = self._summarize_retrieved_data(retrieved_data)
        if app_summary["apps"]:
            response_parts.append("\nğŸ“± åº”ç”¨åˆ†å¸ƒï¼š")
            for app, count in sorted(app_summary["apps"].items(), key=lambda x: x[1], reverse=True):
                response_parts.append(f"  â€¢ {app}: {count} æ¡è®°å½•")

        # æ—¶é—´èŒƒå›´
        if app_summary["time_range"]:
            try:
                earliest = datetime.fromisoformat(
                    app_summary["time_range"]["earliest"].replace("Z", "+00:00")
                )
                latest = datetime.fromisoformat(
                    app_summary["time_range"]["latest"].replace("Z", "+00:00")
                )
                response_parts.append(
                    f"\nâ° æ—¶é—´èŒƒå›´: {earliest.strftime('%Y-%m-%d %H:%M')} è‡³ {latest.strftime('%Y-%m-%d %H:%M')}"
                )
            except:  # noqa: E722
                pass

        # æœ€æ–°è®°å½•ç¤ºä¾‹
        if retrieved_data:
            response_parts.append("\nğŸ“ æœ€æ–°è®°å½•ç¤ºä¾‹ï¼š")
            latest_record = retrieved_data[0]
            timestamp = latest_record.get("timestamp", "æœªçŸ¥æ—¶é—´")
            app_name = latest_record.get("app_name", "æœªçŸ¥åº”ç”¨")
            ocr_text = latest_record.get("ocr_text", "æ— å†…å®¹")[:100]

            response_parts.append(f"  æ—¶é—´: {timestamp}")
            response_parts.append(f"  åº”ç”¨: {app_name}")
            response_parts.append(f"  å†…å®¹: {ocr_text}...")

        response_parts.append("\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯æ¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœã€‚")

        return "\n".join(response_parts)

    def health_check(self) -> dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥

        Returns:
            æœåŠ¡çŠ¶æ€ä¿¡æ¯
        """
        return {
            "rag_service": "healthy",
            "llm_client": ("available" if self.llm_client.is_available() else "unavailable"),
            "database": "connected",
            "components": {
                "retrieval_service": "ready",
                "context_builder": "ready",
                "query_parser": "ready",
            },
            "timestamp": get_utc_now().isoformat(),
        }

    def _generate_direct_response(self, user_query: str, intent_result: dict[str, Any]) -> str:
        """
        ä¸ºä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„ç”¨æˆ·è¾“å…¥ç”Ÿæˆç›´æ¥å›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        try:
            intent_type = intent_result.get("intent_type", "general_chat")

            if intent_type == "system_help":
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ã€‚LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æˆªå›¾è®°å½•ç”¨æˆ·çš„å±å¹•æ´»åŠ¨
2. OCRæ–‡å­—è¯†åˆ«å’Œå†…å®¹åˆ†æ
3. åº”ç”¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡
4. æ™ºèƒ½æœç´¢å’ŒæŸ¥è¯¢åŠŸèƒ½

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰ç”¨çš„å¸®åŠ©ä¿¡æ¯ã€‚
"""
            else:
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ä»¥å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
å¦‚æœç”¨æˆ·éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å¼•å¯¼ä»–ä»¬ä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢è¯­å¥ã€‚
"""

            response = self.llm_client.client.chat.completions.create(
                model=self.llm_client.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ],
                temperature=0.7,
                max_tokens=500,
            )

            # è®°å½•LLMå“åº”åˆ°æ—¥å¿—
            llm_response = response.choices[0].message.content.strip()
            logger.info(f"[LLM Direct Response] {llm_response}")
            logger.info(f"LLMç›´æ¥å“åº”: {llm_response}")

            return llm_response

        except Exception as e:
            logger.error(f"ç›´æ¥å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_direct_response(user_query, intent_result)

    def _fallback_direct_response(self, user_query: str, intent_result: dict[str, Any]) -> str:
        """
        å½“LLMä¸å¯ç”¨æ—¶çš„ç›´æ¥å›å¤å¤‡ç”¨æ–¹æ¡ˆ

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            å¤‡ç”¨å›å¤æ–‡æœ¬
        """
        intent_type = intent_result.get("intent_type", "general_chat")

        if intent_type == "system_help":
            return """
LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

ğŸ“¸ **è‡ªåŠ¨æˆªå›¾è®°å½•**
- å®šæœŸæ•è·å±å¹•å†…å®¹
- è®°å½•åº”ç”¨ä½¿ç”¨æƒ…å†µ

ğŸ” **æ™ºèƒ½æœç´¢**
- æœç´¢å†å²æˆªå›¾
- åŸºäºOCRæ–‡å­—å†…å®¹æŸ¥æ‰¾

ğŸ“Š **ä½¿ç”¨ç»Ÿè®¡**
- åº”ç”¨ä½¿ç”¨æ—¶é•¿ç»Ÿè®¡
- æ´»åŠ¨æ¨¡å¼åˆ†æ

ğŸ’¬ **æ™ºèƒ½é—®ç­”**
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- ä¸ªæ€§åŒ–æ•°æ®åˆ†æ

å¦‚éœ€æŸ¥è¯¢å…·ä½“æ•°æ®ï¼Œè¯·ä½¿ç”¨å¦‚"æœç´¢åŒ…å«ç¼–ç¨‹çš„æˆªå›¾"æˆ–"ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨çš„åº”ç”¨ä½¿ç”¨æƒ…å†µ"ç­‰è¯­å¥ã€‚
"""
        elif intent_type == "general_chat":
            greetings = [
                "ä½ å¥½ï¼æˆ‘æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼",
                "æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "æ¬¢è¿ä½¿ç”¨LifeTraceï¼æˆ‘å¯ä»¥å¸®æ‚¨æŸ¥è¯¢å’Œåˆ†ææ‚¨çš„ç”Ÿæ´»è½¨è¿¹æ•°æ®ã€‚",
            ]

            if any(word in user_query.lower() for word in ["ä½ å¥½", "hello", "hi"]):
                return greetings[0] + "\n\næ‚¨å¯ä»¥è¯¢é—®æˆ‘å…³äºLifeTraceçš„åŠŸèƒ½ï¼Œæˆ–è€…ç›´æ¥æŸ¥è¯¢æ‚¨çš„æ•°æ®ã€‚"
            elif any(word in user_query.lower() for word in ["è°¢è°¢", "thanks"]):
                return "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ã€‚"
            else:
                return greetings[1] + "\n\næ‚¨å¯ä»¥å°è¯•æœç´¢æˆªå›¾ã€æŸ¥è¯¢åº”ç”¨ä½¿ç”¨æƒ…å†µï¼Œæˆ–è€…è¯¢é—®ç³»ç»ŸåŠŸèƒ½ã€‚"
        else:
            return "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½æä¾›å‡†ç¡®çš„å›ç­”ã€‚æ‚¨å¯ä»¥å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢ï¼Œæ¯”å¦‚æœç´¢ç‰¹å®šå†…å®¹æˆ–ç»Ÿè®¡ä½¿ç”¨æƒ…å†µã€‚"

    def _get_task_status_emoji(self, status: str) -> str:
        """è·å–ä»»åŠ¡çŠ¶æ€å¯¹åº”çš„ emoji"""
        return {
            "pending": "â³",
            "in_progress": "ğŸ”„",
            "completed": "âœ…",
            "cancelled": "âŒ",
        }.get(status, "ğŸ“")

    def _format_task_line(self, task: dict, truncate_desc: bool = True) -> str:
        """æ ¼å¼åŒ–å•ä¸ªä»»åŠ¡è¡Œ"""
        MAX_TASK_DESCRIPTION_LENGTH = 50
        status = task.get("status", "pending")
        status_emoji = self._get_task_status_emoji(status)
        task_line = f"{status_emoji} [{status}] {task.get('name', 'æœªå‘½åä»»åŠ¡')}"

        if task.get("description"):
            description = task.get("description")
            if truncate_desc and len(description) > MAX_TASK_DESCRIPTION_LENGTH:
                description = description[:MAX_TASK_DESCRIPTION_LENGTH] + "..."
            task_line += f"\n   æè¿°: {description}"
        return task_line

    def _get_project_tasks_info(
        self, project_id: int, task_ids: list[int] | None
    ) -> tuple[dict | None, str, str | None]:
        """è·å–é¡¹ç›®å’Œä»»åŠ¡ä¿¡æ¯"""
        project_info = project_mgr.get_project(project_id)
        logger.info(f"[stream] è·å–åˆ°é¡¹ç›®ä¿¡æ¯: {project_info}")

        tasks_info_str = "æš‚æ— ä»»åŠ¡"
        selected_tasks_info_str = None

        # è·å–æ‰€æœ‰ä»»åŠ¡
        tasks = task_mgr.list_tasks(project_id, limit=100)
        if tasks:
            tasks_info_str = "\n".join(
                self._format_task_line(task, truncate_desc=True) for task in tasks
            )
            logger.info(f"[stream] è·å–åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
        else:
            logger.info(f"[stream] é¡¹ç›® {project_id} æš‚æ— ä»»åŠ¡")

        # è·å–é€‰ä¸­ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
        if task_ids:
            selected_tasks = []
            for task_id in task_ids:
                task = task_mgr.get_task(task_id)
                if task:
                    selected_tasks.append(self._format_task_line(task, truncate_desc=False))
            if selected_tasks:
                selected_tasks_info_str = "\n\n".join(selected_tasks)
                logger.info(f"[stream] è·å–åˆ° {len(selected_tasks)} ä¸ªé€‰ä¸­çš„ä»»åŠ¡")

        return project_info, tasks_info_str, selected_tasks_info_str

    def _append_history_messages(self, messages: list, session_id: str, history_limit: int) -> None:
        """æ·»åŠ å†å²å¯¹è¯æ¶ˆæ¯"""
        try:
            history_messages = chat_mgr.get_messages(session_id, limit=history_limit * 2)
            for msg in history_messages:
                if msg["role"] in ["user", "assistant"]:
                    messages.append({"role": msg["role"], "content": msg["content"]})
            if history_messages:
                logger.info(f"[stream] æ·»åŠ äº† {len(history_messages)} æ¡å†å²æ¶ˆæ¯")
        except Exception as e:
            logger.warning(f"[stream] è·å–å†å²æ¶ˆæ¯å¤±è´¥: {e}")

    def _get_system_prompt_for_project(
        self,
        project_info: dict,
        tasks_info_str: str,
        selected_tasks_info_str: str | None,
        with_data: bool = False,
    ) -> str:
        """è·å–é¡¹ç›®å¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯"""
        project_name = project_info.get("name", "æœªå‘½åé¡¹ç›®")
        project_goal = project_info.get("goal", "æš‚æ— ç›®æ ‡æè¿°")

        if with_data:
            if selected_tasks_info_str:
                return get_prompt(
                    "project_assistant",
                    "system_prompt_with_data_and_selected_tasks",
                    project_name=project_name,
                    project_goal=project_goal,
                    selected_tasks_info=selected_tasks_info_str,
                    tasks_info=tasks_info_str,
                )
            return get_prompt(
                "project_assistant",
                "system_prompt_with_data",
                project_name=project_name,
                project_goal=project_goal,
                tasks_info=tasks_info_str,
            )

        if selected_tasks_info_str:
            return get_prompt(
                "project_assistant",
                "system_prompt_with_selected_tasks",
                project_name=project_name,
                project_goal=project_goal,
                selected_tasks_info=selected_tasks_info_str,
                tasks_info=tasks_info_str,
            )
        return get_prompt(
            "project_assistant",
            "system_prompt",
            project_name=project_name,
            project_goal=project_goal,
            tasks_info=tasks_info_str,
        )

    def _build_messages_without_db(
        self,
        user_query: str,
        intent_result: dict,
        project_info: dict | None,
        tasks_info_str: str,
        selected_tasks_info_str: str | None,
    ) -> list[dict]:
        """æ„å»ºä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æ¶ˆæ¯"""
        intent_type = intent_result.get("intent_type", "general_chat")

        if project_info:
            system_prompt = self._get_system_prompt_for_project(
                project_info, tasks_info_str, selected_tasks_info_str, with_data=False
            )
        elif intent_type == "system_help":
            system_prompt = get_prompt("rag", "system_help")
        else:
            system_prompt = get_prompt("rag", "general_chat")

        return [{"role": "system", "content": system_prompt}]

    def _build_messages_with_db(
        self,
        user_query: str,
        project_id: int | None,
        project_info: dict | None,
        tasks_info_str: str,
        selected_tasks_info_str: str | None,
    ) -> list[dict]:
        """æ„å»ºéœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æ¶ˆæ¯"""
        parsed_query = self.query_parser.parse_query(user_query)
        if project_id:
            parsed_query.project_id = project_id

        query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
        retrieved_data = self.retrieval_service.search_by_conditions(parsed_query, 500)

        # æ„å»ºä¸Šä¸‹æ–‡
        if query_type == "statistics":
            stats = None
            if isinstance(parsed_query, QueryConditions):
                stats = self.retrieval_service.get_statistics(parsed_query)
            context_text = self.context_builder.build_statistics_context(
                user_query, retrieved_data, stats
            )
        else:
            context_text = self.context_builder.build_search_context(user_query, retrieved_data)
        logger.debug(f"æ„å»ºçš„ä¸Šä¸‹æ–‡å†…å®¹: {context_text}")

        # ç¡®å®šç³»ç»Ÿå†…å®¹
        if project_info:
            project_context = self._get_system_prompt_for_project(
                project_info, tasks_info_str, selected_tasks_info_str, with_data=True
            )
            system_content = f"{project_context}\n\n{context_text}"
        else:
            system_content = context_text

        return [{"role": "system", "content": system_content}]

    async def process_query_stream(
        self,
        user_query: str,
        project_id: int | None = None,
        task_ids: list[int] | None = None,
        session_id: str | None = None,
    ) -> dict[str, Any]:
        """ä¸ºæµå¼æ¥å£å¤„ç†æŸ¥è¯¢ï¼Œè¿”å›æ„å»ºå¥½çš„messageså’Œtemperature"""
        try:
            logger.info(
                f"[stream] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}, project_id: {project_id}, "
                f"task_ids: {task_ids}, session_id: {session_id}"
            )
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            # è·å–å†å²å¯¹è¯é…ç½®
            enable_history = settings.chat.enable_history
            history_limit = settings.chat.history_limit

            # è·å–é¡¹ç›®å’Œä»»åŠ¡ä¿¡æ¯
            project_info, tasks_info_str, selected_tasks_info_str = None, "æš‚æ— ä»»åŠ¡", None
            if project_id:
                project_info, tasks_info_str, selected_tasks_info_str = (
                    self._get_project_tasks_info(project_id, task_ids)
                )

            # æ„å»ºæ¶ˆæ¯
            if needs_db:
                messages = self._build_messages_with_db(
                    user_query, project_id, project_info, tasks_info_str, selected_tasks_info_str
                )
                temperature = 0.3
            else:
                messages = self._build_messages_without_db(
                    user_query, intent_result, project_info, tasks_info_str, selected_tasks_info_str
                )
                temperature = 0.7

            # æ·»åŠ å†å²å¯¹è¯
            if enable_history and session_id and history_limit > 0:
                self._append_history_messages(messages, session_id, history_limit)

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": user_query})

            return {
                "success": True,
                "messages": messages,
                "temperature": temperature,
                "intent_result": intent_result,
            }

        except Exception as e:
            logger.error(f"[stream] å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "response": f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "messages": [],
                "temperature": 0.7,
            }
