"use client";

import React, { useEffect, useRef, useState } from 'react';
import OpenAI from 'openai';
import WaveformTimeline from './components/WaveformTimeline';
import TranscriptionLog from './components/TranscriptionLog';
import ChatInterface from './components/ChatInterface';
import { useAppStore } from './store/useAppStore';
import { RecordingService } from './services/RecordingService';
import { RecognitionService } from './services/RecognitionService';
import { WebSocketRecognitionService } from './services/WebSocketRecognitionService';
import { OptimizationService } from './services/OptimizationService';
import { ScheduleExtractionService } from './services/ScheduleExtractionService';
import { PersistenceService } from './services/PersistenceService';
import { TranscriptSegment, ChatMessage, AudioSegment, ScheduleItem } from './types';

const SYSTEM_PROMPT_CHAT = `
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æœ€è¿‘10åˆ†é’Ÿçš„è¯­éŸ³è½¬å½•ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
`;

export function VoiceModulePanel() {
  const {
    isRecording,
    recordingStartTime,
    currentTime,
    timeline,
    transcripts,
    schedules,
    audioSegments,
    processStatus,
    startRecording: storeStartRecording,
    stopRecording: storeStopRecording,
    setCurrentTime: storeSetCurrentTime,
    setTimelineView,
    setTimelineZoom,
    addTranscript,
    updateTranscript,
    addSchedule,
    addAudioSegment,
    updateAudioSegment,
    setProcessStatus,
  } = useAppStore();

  const recordingServiceRef = useRef<RecordingService | null>(null);
  const recognitionServiceRef = useRef<RecognitionService | null>(null);
  const websocketRecognitionServiceRef = useRef<WebSocketRecognitionService | null>(null);
  const optimizationServiceRef = useRef<OptimizationService | null>(null);
  const scheduleExtractionServiceRef = useRef<ScheduleExtractionService | null>(null);
  const persistenceServiceRef = useRef<PersistenceService | null>(null);

  const [chatMessages, setChatMessages] = useState<ChatMessage[]>([
    { 
      id: 'init', 
      role: 'model', 
      text: 'ä½ å¥½ï¼æˆ‘æ˜¯åŸºäº DeepSeek çš„ 7Ã—24 æ™ºèƒ½å½•éŸ³åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥æŒç»­å½•éŸ³ã€è¯†åˆ«è¯­éŸ³å¹¶è‡ªåŠ¨æå–æ—¥ç¨‹ã€‚ä½ å¯ä»¥éšæ—¶å‘æˆ‘æé—®ï¼Œå¦‚æœæœ‰å½•éŸ³å†…å®¹ï¼Œæˆ‘ä¼šåŸºäºæœ€è¿‘çš„å½•éŸ³å†…å®¹å›ç­”ï¼›å¦‚æœæ²¡æœ‰å½•éŸ³å†…å®¹ï¼Œæˆ‘ä¹Ÿå¯ä»¥å›ç­”ä¸€èˆ¬æ€§é—®é¢˜ã€‚', 
      timestamp: new Date() 
    }
  ]);
  const [isChatLoading, setIsChatLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isApiKeyMissing, setIsApiKeyMissing] = useState(false);
  const [analyser, setAnalyser] = useState<AnalyserNode | null>(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [audioSource, setAudioSource] = useState<'microphone' | 'system'>('microphone');

  const audioPlayerRef = useRef<HTMLAudioElement | null>(null);
  const playbackIntervalRef = useRef<number | null>(null);

  // åˆå§‹åŒ–æœåŠ¡
  useEffect(() => {
    const recordingService = new RecordingService(audioSource);
    recordingService.setCallbacks({
      onSegmentReady: handleAudioSegmentReady,
      onError: (err) => {
        console.error('Recording error:', err);
        setError(err.message);
        setProcessStatus('recording', 'error');
      },
      onAudioData: (analyserNode) => {
        setAnalyser(analyserNode);
      },
    });
    recordingServiceRef.current = recordingService;

    // Web Speech API è¯†åˆ«æœåŠ¡ï¼ˆç”¨äºéº¦å…‹é£ï¼‰
    const recognitionService = new RecognitionService();
    recognitionService.setCallbacks({
      onResult: handleRecognitionResult,
      onError: (err) => {
        console.error('Recognition error:', err);
        setError(err.message);
        setProcessStatus('recognition', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('recognition', status);
      },
    });
    recognitionServiceRef.current = recognitionService;
    
    // WebSocket Faster-Whisper è¯†åˆ«æœåŠ¡ï¼ˆç”¨äºç³»ç»ŸéŸ³é¢‘å’Œé«˜è´¨é‡è¯†åˆ«ï¼‰
    const websocketRecognitionService = new WebSocketRecognitionService();
    websocketRecognitionService.setCallbacks({
      onResult: handleRecognitionResult,
      onError: (err) => {
        console.error('WebSocket recognition error:', err);
        setError(err.message);
        setProcessStatus('recognition', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('recognition', status);
      },
    });
    websocketRecognitionServiceRef.current = websocketRecognitionService;

    const optimizationService = new OptimizationService();
    optimizationService.setCallbacks({
      onOptimized: handleTextOptimized,
      onError: (segmentId, err) => {
        console.error(`Optimization error for ${segmentId}:`, err);
        setProcessStatus('optimization', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('optimization', status);
      },
    });
    optimizationServiceRef.current = optimizationService;

    const scheduleExtractionService = new ScheduleExtractionService();
    scheduleExtractionService.setCallbacks({
      onScheduleExtracted: handleScheduleExtracted,
      onError: (err) => {
        console.error('Schedule extraction error:', err);
        setProcessStatus('scheduleExtraction', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('scheduleExtraction', status);
      },
    });
    scheduleExtractionServiceRef.current = scheduleExtractionService;

    const persistenceService = new PersistenceService();
    persistenceService.setCallbacks({
      onUploadProgress: (type, progress) => {
        console.log(`Upload progress (${type}): ${progress}%`);
      },
      onError: (err) => {
        console.error('Persistence error:', err);
        setProcessStatus('persistence', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('persistence', status);
      },
    });
    persistenceServiceRef.current = persistenceService;

    const audio = new Audio();
    audioPlayerRef.current = audio;
    audio.onended = () => {
      setIsPlaying(false);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
    };
    audio.onpause = () => {
      setIsPlaying(false);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
    };
    audio.onplay = () => {
      setIsPlaying(true);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
      playbackIntervalRef.current = window.setInterval(() => {
        if (audio.currentTime) {
          storeSetCurrentTime(new Date(Date.now() - (audio.duration - audio.currentTime) * 1000));
        }
      }, 100);
    };

    let apiKey = process.env.NEXT_PUBLIC_DEEPSEEK_API_KEY;
    if (!apiKey || apiKey.includes('your_deepseek_api_key')) {
      apiKey = "sk-26d76c61cf2842fcb729e019d587a026";
    }
    setIsApiKeyMissing(!apiKey);

    return () => {
      recordingService.stop();
      recognitionService.stop();
      websocketRecognitionService.stop();
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
      audio.pause();
    };
  }, [audioSource]);

  useEffect(() => {
    const interval = setInterval(() => {
      storeSetCurrentTime(new Date());
    }, 1000);
    return () => clearInterval(interval);
  }, []);

  const handleAudioSegmentReady = async (blob: Blob, startTime: Date, endTime: Date, segmentId: string, source: 'microphone' | 'system') => {
    const audioSegment: AudioSegment = {
      id: segmentId,
      startTime,
      endTime,
      duration: endTime.getTime() - startTime.getTime(),
      fileSize: blob.size,
      audioSource: source,
      uploadStatus: 'pending',
    };
    addAudioSegment(audioSegment);

    if (persistenceServiceRef.current) {
      updateAudioSegment(segmentId, { uploadStatus: 'uploading' });
      const audioFileId = await persistenceServiceRef.current.uploadAudio(blob, {
        startTime,
        endTime,
        segmentId,
      });
      if (audioFileId) {
        // è·å–éŸ³é¢‘æ–‡ä»¶ URL
        const audioUrl = await persistenceServiceRef.current.getAudioUrl(audioFileId);
        updateAudioSegment(segmentId, { 
          fileUrl: audioUrl || undefined, 
          uploadStatus: 'uploaded' 
        });
      } else {
        updateAudioSegment(segmentId, { uploadStatus: 'failed' });
      }
    }
  };

  const handleRecognitionResult = (text: string, isFinal: boolean) => {
    if (!text.trim()) return;
    const currentRecordingStartTime = useAppStore.getState().recordingStartTime;
    if (!currentRecordingStartTime) return;

    const now = Date.now();
    const relativeEndTime = now - currentRecordingStartTime.getTime();
    const relativeStartTime = Math.max(0, relativeEndTime - 2000);
    const absoluteEnd = new Date();
    const absoluteStart = new Date(absoluteEnd.getTime() - Math.max(500, relativeEndTime - relativeStartTime));

    const lastSegment = useAppStore.getState().audioSegments[useAppStore.getState().audioSegments.length - 1];
    const transcripts = useAppStore.getState().transcripts;
    
    if (isFinal) {
      // æœ€ç»ˆç»“æœï¼šæ€»æ˜¯åˆ›å»ºæ–°ç‰‡æ®µï¼ˆä¿ç•™å†å²è®°å½•ï¼‰
      // æ£€æŸ¥æ˜¯å¦ä¸æœ€åä¸€ä¸ªæœ€ç»ˆç‰‡æ®µå†…å®¹ç›¸åŒï¼ˆé¿å…é‡å¤ï¼‰
      const lastFinalSegment = [...transcripts].reverse().find(t => !t.isInterim);
      if (lastFinalSegment && lastFinalSegment.rawText === text) {
        // å†…å®¹ç›¸åŒï¼Œå¯èƒ½æ˜¯é‡å¤å‘é€ï¼Œè·³è¿‡
        console.log('è·³è¿‡é‡å¤çš„è¯†åˆ«ç»“æœ:', text);
        return;
      }
      
      // å¦‚æœæœ‰ä¸´æ—¶ç‰‡æ®µï¼Œå…ˆå°†å…¶è½¬ä¸ºæœ€ç»ˆç»“æœ
      const lastInterimSegment = [...transcripts].reverse().find(t => t.isInterim);
      if (lastInterimSegment && lastInterimSegment.interimText === text) {
        // ä¸´æ—¶ç‰‡æ®µå†…å®¹ä¸æœ€ç»ˆç»“æœç›¸åŒï¼Œç›´æ¥è½¬ä¸ºæœ€ç»ˆç»“æœ
        updateTranscript(lastInterimSegment.id, {
          rawText: text,
          isInterim: false,
          interimText: undefined,
          absoluteStart,
          absoluteEnd,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
        });
        
        // è§¦å‘ä¼˜åŒ–
        const updatedSegment = { ...lastInterimSegment, rawText: text, isInterim: false };
        if (optimizationServiceRef.current) {
          optimizationServiceRef.current.enqueue(updatedSegment);
        }
      } else {
        // åˆ›å»ºæ–°çš„æœ€ç»ˆç‰‡æ®µ
        const segment: TranscriptSegment = {
          id: `transcript_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`,
          timestamp: new Date(),
          absoluteStart,
          absoluteEnd,
          segmentId: lastSegment?.id,
          rawText: text,
          isOptimized: false,
          isInterim: false,
          containsSchedule: false,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
          uploadStatus: 'pending',
        };
        addTranscript(segment);
        if (optimizationServiceRef.current) {
          optimizationServiceRef.current.enqueue(segment);
        }
      }
    } else {
      // ä¸´æ—¶ç»“æœï¼šæ›´æ–°æœ€åä¸€ä¸ªä¸´æ—¶ç‰‡æ®µæˆ–åˆ›å»ºæ–°çš„ä¸´æ—¶ç‰‡æ®µ
      const lastInterimSegment = [...transcripts].reverse().find(t => t.isInterim);
      
      if (lastInterimSegment) {
        // æ›´æ–°ä¸´æ—¶æ–‡æœ¬
        updateTranscript(lastInterimSegment.id, {
          interimText: text,
          absoluteEnd,
          audioEnd: relativeEndTime,
        });
      } else {
        // åˆ›å»ºæ–°çš„ä¸´æ—¶ç‰‡æ®µ
        const segment: TranscriptSegment = {
          id: `transcript_interim_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`,
          timestamp: new Date(),
          absoluteStart,
          absoluteEnd,
          segmentId: lastSegment?.id,
          rawText: '', // ä¸´æ—¶ç»“æœæ—¶ä¸ºç©º
          interimText: text,
          isOptimized: false,
          isInterim: true,
          containsSchedule: false,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
          uploadStatus: 'pending',
        };
        addTranscript(segment);
      }
    }
  };

  const handleTextOptimized = (segmentId: string, optimizedText: string, containsSchedule: boolean) => {
    updateTranscript(segmentId, {
      optimizedText,
      isOptimized: true,
      containsSchedule,
    });

    if (containsSchedule && scheduleExtractionServiceRef.current) {
      const segment = transcripts.find(t => t.id === segmentId);
      if (segment) {
        scheduleExtractionServiceRef.current.enqueue({
          ...segment,
          optimizedText,
          isOptimized: true,
          containsSchedule,
        });
      }
    }

    setTimeout(() => {
      const currentTranscripts = useAppStore.getState().transcripts;
      const pendingTranscripts = currentTranscripts.filter(t => t.isOptimized && t.uploadStatus === 'pending');
      if (pendingTranscripts.length >= 10 && persistenceServiceRef.current) {
        persistenceServiceRef.current.saveTranscripts(pendingTranscripts);
        pendingTranscripts.forEach(t => {
          updateTranscript(t.id, { uploadStatus: 'uploaded' });
        });
      }
    }, 100);
  };

  const handleScheduleExtracted = (schedule: ScheduleItem) => {
    addSchedule(schedule);
    setTimeout(() => {
      const currentSchedules = useAppStore.getState().schedules;
      const pendingSchedules = currentSchedules.filter(s => s.status === 'pending');
      if (pendingSchedules.length >= 5 && persistenceServiceRef.current) {
        persistenceServiceRef.current.saveSchedules(pendingSchedules);
      }
    }, 100);
  };

  const handleStartRecording = async () => {
    setError(null);
    storeStartRecording();
    try {
      if (recordingServiceRef.current) {
        // æ›´æ–°éŸ³é¢‘æº
        recordingServiceRef.current.setAudioSource(audioSource);
        
        // å¦‚æœæ˜¯ç³»ç»ŸéŸ³é¢‘ï¼Œæç¤ºç”¨æˆ·
        if (audioSource === 'system') {
          // æµè§ˆå™¨ä¼šè‡ªåŠ¨å¼¹å‡ºé€‰æ‹©çª—å£ï¼Œè¿™é‡Œå¯ä»¥æ·»åŠ æç¤º
          console.log('è¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©è¦å…±äº«çš„æ ‡ç­¾é¡µï¼ˆåŒ…å«éŸ³é¢‘ï¼‰');
        }
        
        await recordingServiceRef.current.start();
        setProcessStatus('recording', 'running');
      }
      // æ ¹æ®éŸ³é¢‘æºé€‰æ‹©è¯†åˆ«æœåŠ¡
      if (audioSource === 'microphone') {
        // éº¦å…‹é£ï¼šä½¿ç”¨ Web Speech APIï¼ˆå¿«é€Ÿã€å…è´¹ï¼‰
        if (recognitionServiceRef.current) {
          setTimeout(() => {
            recognitionServiceRef.current?.start();
          }, 500);
        }
      } else if (audioSource === 'system') {
        // ç³»ç»ŸéŸ³é¢‘ï¼šä½¿ç”¨ WebSocket Faster-Whisperï¼ˆæ”¯æŒç³»ç»ŸéŸ³é¢‘ï¼Œæ›´å‡†ç¡®ï¼‰
        // ç­‰å¾…å½•éŸ³æœåŠ¡å®Œå…¨å¯åŠ¨åå†è·å–æµ
        setTimeout(() => {
          if (recordingServiceRef.current && websocketRecognitionServiceRef.current) {
            const stream = recordingServiceRef.current.getStream();
            if (stream) {
              console.log('å¯åŠ¨ WebSocket Faster-Whisper è¯†åˆ«...');
              websocketRecognitionServiceRef.current.start(stream);
            } else {
              console.warn('æ— æ³•è·å–éŸ³é¢‘æµï¼ŒWebSocket è¯†åˆ«æœªå¯åŠ¨');
              setError('æ— æ³•è·å–éŸ³é¢‘æµï¼Œè¯·é‡è¯•');
            }
          }
        }, 1000); // ç­‰å¾…å½•éŸ³æœåŠ¡å®Œå…¨å¯åŠ¨
      }
    } catch (err) {
      const error = err instanceof Error ? err : new Error('Failed to start recording');
      console.error('Recording error:', error);
      setError(error.message);
      setProcessStatus('recording', 'error');
      storeStopRecording();
    }
  };

  const handleStopRecording = async () => {
    if (recordingServiceRef.current) {
      await recordingServiceRef.current.stop();
      setProcessStatus('recording', 'idle');
    }
    if (recognitionServiceRef.current) {
      recognitionServiceRef.current.stop();
    }
    if (websocketRecognitionServiceRef.current) {
      websocketRecognitionServiceRef.current.stop();
    }
    storeStopRecording();
  };

  const handleSeek = async (time: Date) => {
    if (isRecording) return;
    const segment = audioSegments.find(s => s.startTime <= time && s.endTime >= time);
    if (segment && audioPlayerRef.current) {
      // å¦‚æœè¿˜æ²¡æœ‰ fileUrlï¼Œå°è¯•è·å–
      let audioUrl = segment.fileUrl;
      if (!audioUrl && segment.id && persistenceServiceRef.current) {
        const fetchedUrl = await persistenceServiceRef.current.getAudioUrl(segment.id);
        if (fetchedUrl) {
          audioUrl = fetchedUrl;
          updateAudioSegment(segment.id, { fileUrl: fetchedUrl });
        }
      }
      
      if (audioUrl) {
        audioPlayerRef.current.src = audioUrl;
        const seekTime = (time.getTime() - segment.startTime.getTime()) / 1000;
        audioPlayerRef.current.currentTime = seekTime;
        audioPlayerRef.current.play();
      }
    }
  };

  const handleTimelineChange = (startTime: Date, duration: number) => {
    setTimelineView(startTime, duration);
    const endTime = new Date(startTime.getTime() + duration);
    if (persistenceServiceRef.current) {
      persistenceServiceRef.current.queryTranscripts(startTime, endTime).then(fetched => {
        const existing = useAppStore.getState().transcripts;
        fetched.forEach(segment => {
          if (!existing.find(t => t.id === segment.id)) {
            addTranscript(segment);
          }
        });
      });
      persistenceServiceRef.current.querySchedules(startTime, endTime).then(fetched => {
        const existing = useAppStore.getState().schedules;
        fetched.forEach(schedule => {
          if (!existing.find(s => s.id === schedule.id)) {
            addSchedule(schedule);
          }
        });
      });
    }
  };

  const handleSegmentClick = async (
    startMs: number,
    endMs: number,
    segmentId?: string,
    absoluteStartMs?: number
  ) => {
    if (isRecording || !recordingStartTime) return;
    let targetSegment = segmentId ? audioSegments.find(s => s.id === segmentId) : undefined;
    if (!targetSegment && absoluteStartMs) {
      const abs = new Date(absoluteStartMs);
      targetSegment = audioSegments.find(s => s.startTime <= abs && s.endTime >= abs);
    }
    if (!targetSegment) {
      const startTime = new Date(recordingStartTime.getTime() + startMs);
      await handleSeek(startTime);
      return;
    }
    if (audioPlayerRef.current) {
      // å¦‚æœè¿˜æ²¡æœ‰ fileUrlï¼Œå°è¯•è·å–
      let audioUrl = targetSegment.fileUrl;
      if (!audioUrl && targetSegment.id && persistenceServiceRef.current) {
        const fetchedUrl = await persistenceServiceRef.current.getAudioUrl(targetSegment.id);
        if (fetchedUrl) {
          audioUrl = fetchedUrl;
          updateAudioSegment(targetSegment.id, { fileUrl: fetchedUrl });
        }
      }
      
      if (audioUrl) {
        audioPlayerRef.current.src = audioUrl;
        let seekSeconds = 0;
        if (absoluteStartMs) {
          seekSeconds = Math.max(0, (absoluteStartMs - targetSegment.startTime.getTime()) / 1000);
        } else {
          seekSeconds = Math.max(0, (startMs - (targetSegment.startTime.getTime() - recordingStartTime.getTime())) / 1000);
        }
        audioPlayerRef.current.currentTime = seekSeconds;
        audioPlayerRef.current.play();
        return;
      }
    }
    const startTime = new Date(recordingStartTime.getTime() + startMs);
    await handleSeek(startTime);
  };

  const handleSendMessage = async (text: string) => {
    if (!text.trim()) return;
    const msgId = Date.now().toString();
    setChatMessages(prev => [...prev, { id: msgId, role: 'user', text, timestamp: new Date() }]);
    setIsChatLoading(true);
    try {
      // è·å–æœ€è¿‘ 10 åˆ†é’Ÿçš„è½¬å½•å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
      const tenMinutesAgo = new Date(Date.now() - 10 * 60 * 1000);
      const contextSegments = transcripts.filter(t => t.timestamp > tenMinutesAgo);
      const contextString = contextSegments
        .map(t => `[${t.timestamp.toLocaleTimeString()}] ${t.optimizedText || t.rawText}`)
        .join('\n');
      
      // ä½¿ç”¨ Next.js ä»£ç†è·¯å¾„ï¼ˆä¼šè‡ªåŠ¨ä»£ç†åˆ°åç«¯ localhost:8000ï¼‰
      // API Key ç”±åç«¯ç®¡ç†ï¼Œå‰ç«¯ä¸éœ€è¦ä¼ é€’çœŸå® Key
      // OpenAI SDK éœ€è¦å®Œæ•´çš„ URLï¼Œåœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ä½¿ç”¨ window.location.origin
      let baseURL: string;
      if (typeof window !== 'undefined' && window.location) {
        // æµè§ˆå™¨ç¯å¢ƒï¼šä½¿ç”¨å½“å‰é¡µé¢çš„ origin + ä»£ç†è·¯å¾„
        const origin = window.location.origin;
        if (!origin || origin === 'null' || origin === 'undefined') {
          // å¦‚æœ origin æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼
          baseURL = 'http://localhost:3000/api/deepseek';
        } else {
          baseURL = `${origin}/api/deepseek`;
        }
      } else {
        // æœåŠ¡ç«¯ç¯å¢ƒï¼šç›´æ¥ä½¿ç”¨åç«¯åœ°å€
        baseURL = 'http://localhost:8000/api/deepseek';
      }
      
      // éªŒè¯ baseURL æ˜¯å¦æœ‰æ•ˆ
      try {
        new URL(baseURL);
      } catch (urlError) {
        console.error('Invalid baseURL:', baseURL, urlError);
        throw new Error(`Invalid API URL: ${baseURL}`);
      }
      
      const ai = new OpenAI({
        baseURL: baseURL,
        apiKey: 'dummy-key', // åç«¯ä¼šä½¿ç”¨é…ç½®çš„ API Keyï¼Œè¿™é‡Œåªæ˜¯å ä½ç¬¦
        dangerouslyAllowBrowser: true,
      });
      
      const timeoutPromise = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('API timeout')), 30000)
      );
      
      // æ„å»ºæ¶ˆæ¯ï¼šå¦‚æœæœ‰ä¸Šä¸‹æ–‡å°±åŠ ä¸Šï¼Œæ²¡æœ‰å°±åªå‘é€ç”¨æˆ·é—®é¢˜
      const systemPrompt = contextString 
        ? SYSTEM_PROMPT_CHAT 
        : 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚ä½ å¯ä»¥å›ç­”ç”¨æˆ·çš„å„ç§é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¯¢é—®å…³äºå½•éŸ³å†…å®¹çš„é—®é¢˜ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ç›®å‰æ²¡æœ‰å½•éŸ³å†…å®¹ï¼Œå»ºè®®å…ˆå¼€å§‹å½•éŸ³ã€‚';
      
      const userContent = contextString
        ? `ä¸Šä¸‹æ–‡ (æœ€è¿‘10åˆ†é’Ÿ):\n${contextString}\n\nç”¨æˆ·æé—®: ${text}`
        : text;
      
      const apiPromise = ai.chat.completions.create({
        model: 'deepseek-chat',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userContent }
        ]
      });
      
      const response = await Promise.race([apiPromise, timeoutPromise]) as any;
      
      setChatMessages(prev => [...prev, {
        id: Date.now().toString(),
        role: 'model',
        text: response.choices?.[0]?.message?.content || "æ— æ³•å¤„ç†è¯¥è¯·æ±‚",
        timestamp: new Date()
      }]);
    } catch (e: any) {
      console.error('Chat API error:', e);
      const errorMessage = e?.message || e?.toString() || "Unknown error";
      setChatMessages(prev => [...prev, { 
        id: Date.now().toString(), 
        role: 'model', 
        text: `å‡ºé”™: ${errorMessage}ã€‚è¯·æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œä»¥åŠ LLM é…ç½®æ˜¯å¦æ­£ç¡®ã€‚`, 
        timestamp: new Date() 
      }]);
    } finally {
      setIsChatLoading(false);
    }
  };

  return (
    <div className="flex h-full flex-col overflow-hidden bg-background text-foreground">
      <header className="shrink-0 border-b border-border bg-card/80 backdrop-blur flex items-center justify-between px-6 py-3 relative">
        <div className="flex items-center gap-2">
          <div className={`w-3 h-3 rounded-full ${isRecording ? 'bg-red-500 animate-pulse' : 'bg-muted'}`}></div>
          <h1 className="font-bold text-lg tracking-tight text-foreground">
            7Ã—24 æ™ºèƒ½å½•éŸ³åŠ©æ‰‹
          </h1>
        </div>
        {error && (
          <div className={`absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 px-4 py-1 rounded-full text-xs font-medium ${
            error.includes('ç³»ç»ŸéŸ³é¢‘æ¨¡å¼') 
              ? 'bg-amber-500/10 text-amber-600 border border-amber-500/20' 
              : 'bg-red-500/10 text-red-600 border border-red-500/20 animate-pulse'
          }`}>
            {error}
          </div>
        )}
        <div className="flex items-center gap-3">
          <div className="flex items-center gap-2 text-xs text-muted-foreground">
            <span className={`w-2 h-2 rounded-full ${processStatus.recording === 'running' ? 'bg-green-500' : 'bg-muted'}`}></span>
            <span>å½•éŸ³</span>
            <span className={`w-2 h-2 rounded-full ${processStatus.recognition === 'running' ? 'bg-green-500' : 'bg-muted'}`}></span>
            <span>è¯†åˆ«</span>
          </div>
          {!isRecording && (
            <select
              value={audioSource}
              onChange={(e) => setAudioSource(e.target.value as 'microphone' | 'system')}
              className="bg-background border border-input text-foreground text-xs px-2 py-1 rounded focus:outline-none focus:ring-2 focus:ring-primary/50"
              title="é€‰æ‹©éŸ³é¢‘æ¥æº"
            >
              <option value="microphone">ğŸ¤ éº¦å…‹é£ï¼ˆWeb Speech APIï¼‰</option>
              <option value="system">ğŸ”Š ç³»ç»ŸéŸ³é¢‘ï¼ˆFaster-Whisper å®æ—¶è¯†åˆ«ï¼‰</option>
            </select>
          )}
          {!isRecording ? (
            <button
              onClick={handleStartRecording}
              className="bg-primary hover:bg-primary/90 text-primary-foreground px-4 py-1.5 rounded-lg text-sm font-medium transition-all shadow-sm flex items-center gap-2"
            >
              å¼€å§‹å½•éŸ³
            </button>
          ) : (
            <button
              onClick={handleStopRecording}
              className="bg-red-500/10 hover:bg-red-500/20 text-red-600 border border-red-500/50 px-4 py-1.5 rounded-lg text-sm font-medium transition-all flex items-center gap-2"
            >
              åœæ­¢å½•éŸ³
            </button>
          )}
        </div>
      </header>
      <main className="flex-1 grid grid-cols-1 md:grid-cols-3 overflow-hidden">
        <div className="md:col-span-2 flex flex-col h-full overflow-hidden border-r border-border">
          <div className="h-64 p-4 shrink-0 bg-card/50 flex flex-col">
            <div className="text-xs text-muted-foreground mb-2 font-mono flex justify-between">
              <span>æ—¶é—´è½´ï¼ˆç»å¯¹æ—¶é—´ï¼‰</span>
              <span>{isRecording ? 'å½•éŸ³ä¸­' : 'ç©ºé—²'}</span>
            </div>
            <div className="flex-1 min-h-0">
              <WaveformTimeline 
                analyser={analyser}
                isRecording={isRecording}
                timeline={timeline}
                audioSegments={audioSegments}
                schedules={schedules}
                onSeek={handleSeek}
                onTimelineChange={handleTimelineChange}
                onZoomChange={setTimelineZoom}
              />
            </div>
          </div>
          <div className="flex-1 flex flex-col min-h-0 bg-background">
            <div className="px-4 py-2 text-xs font-semibold text-muted-foreground uppercase tracking-wider flex justify-between items-center bg-card/50 border-b border-border">
              <span>è½¬å½•æ–‡æœ¬ï¼ˆç‚¹å‡»æ–‡æœ¬å›æ”¾ï¼‰</span>
            </div>
            <TranscriptionLog 
              segments={transcripts} 
              onSegmentClick={handleSegmentClick}
              isRecording={isRecording}
            />
          </div>
        </div>
        <div className="md:col-span-1 h-full min-h-0 overflow-hidden flex flex-col border-l border-border">
          <div className="h-48 p-4 border-b border-border bg-card/50 overflow-y-auto">
            <h3 className="text-sm font-semibold text-foreground mb-2">æ—¥ç¨‹åˆ—è¡¨</h3>
            <div className="space-y-2">
              {schedules.length === 0 ? (
                <p className="text-xs text-muted-foreground">æš‚æ— æ—¥ç¨‹</p>
              ) : (
                schedules.map(schedule => (
                  <div key={schedule.id} className="text-xs bg-card border border-border p-2 rounded">
                    <div className="text-amber-600 font-mono">
                      {schedule.scheduleTime.toLocaleString('zh-CN')}
                    </div>
                    <div className="text-foreground mt-1">{schedule.description}</div>
                  </div>
                ))
              )}
            </div>
          </div>
          <div className="flex-1 min-h-0">
            <ChatInterface 
              messages={chatMessages} 
              onSendMessage={handleSendMessage} 
              isLoading={isChatLoading} 
            />
          </div>
        </div>
      </main>
    </div>
  );
}

